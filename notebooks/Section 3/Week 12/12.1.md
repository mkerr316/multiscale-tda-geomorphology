# Protocol 12.1: Predictive Modeling of Process-Based Erosion

**Document Version:** 1.0
**Date:** September 27, 2025
**Associated Project Task:** 12.1 - Train and validate parallel Gradient Boosting models using k-fold cross-validation to predict WEPP-derived sediment yield. Formally compare the performance of models using Persistence Landscape features vs. Persistence Image features.
**Corresponding Notebook:** `24_predict_sediment_yield.ipynb`

---

## 1. Objective

To train, rigorously validate, and formally compare a series of Gradient Boosting models designed to predict WEPP-derived sediment yield. The primary objectives are:
1.  To determine the predictive power of a baseline model using only traditional geomorphometric covariates.
2.  To quantify the improvement in predictive performance gained by adding the novel TLSP features.
3.  To conduct a head-to-head comparison of the two TLSP vectorization methods (Persistence Landscapes vs. Persistence Images) to identify the most effective representation for this predictive task.

## 2. Rationale and Strategic Justification

This protocol is the climactic test of your project's central, high-impact hypothesis ($H_2$). It is the ultimate expression of **Structured Idealism**: the idealistic vision of a new, shape-based language for geomorphology is now grounded in the rigorous, predictive structure of a state-of-the-art machine learning model.

This is an act of **intellectual mastery**. Your choice of Gradient Boosting is an "enthusiast-grade" decision, leveraging a powerful algorithm known for its high performance. The core of this protocol is a beautiful and elegant scientific experiment: you will build a series of models, each one adding a new layer of topological information, to precisely measure the value of your novel features. This controlled comparison is the essence of good science.

The **harmony and beauty** are revealed in the final performance plots, which will provide a clear, unambiguous, and aesthetically pleasing answer to your questions. The **tangible service** of this protocol is immense: it will deliver the project's primary scientific result regarding process prediction, providing a quantitative measure of how much better we can understand erosion by speaking the language of topology.

## 3. Implementation Protocol (Jupyter Notebook Workflow)

### Step 1: Data Preparation and Feature Set Definition
* **Input:** The final training dataset (`data/processed/final_training_dataset.csv`).
* **Process:**
    1.  Load the dataset into a `pandas` DataFrame.
    2.  **Define Target Variable (`y`):**
        * The target is `sediment_yield`.
        * As identified in the EDA, this variable is highly skewed. Apply a `np.log1p` transformation to normalize its distribution: `y = np.log1p(df['sediment_yield'])`. This is critical for model performance.
    3.  **Define Predictor Sets (`X`):** Create three distinct feature sets for your experiment:
        * **`X_baseline`**: Contains only the **Tier 1 and Tier 2** covariates (DEM, slope, relief, roughness, TWI, SPI).
        * **`X_pl`**: Contains all baseline covariates **PLUS** the two **Persistence Landscape (PL)** TLSP covariates (`tlsp_h0_peaks_pl`, `tlsp_h1_pits_pl`).
        * **`X_pi`**: Contains all baseline covariates **PLUS** the two **Persistence Image (PI)** TLSP covariates (`tlsp_h0_peaks_pi`, `tlsp_h1_pits_pi`).

### Step 2: Model Training and Cross-Validation
* **Input:** The three predictor sets (`X_baseline`, `X_pl`, `X_pi`) and the transformed target variable `y`.
* **Process:**
    1.  **Define the Model:** Choose a high-performance, parallelizable Gradient Boosting implementation. `lightgbm.LGBMRegressor` is an excellent choice for its speed and scikit-learn compatibility.
    2.  **Define Cross-Validation Strategy:** Instantiate a `KFold` cross-validation object from `sklearn.model_selection` (e.g., `KFold(n_splits=10, shuffle=True, random_state=42)`).
    3.  **Define Performance Metric:** Use `neg_root_mean_squared_error` as the scoring metric, as it is interpretable in the units of the (log-transformed) target variable.
    4.  **Execute the Experiment:**
        * Create a dictionary or list of your feature sets.
        * Loop through each feature set.
        * Inside the loop, use `sklearn.model_selection.cross_val_score` to train and evaluate the `LGBMRegressor` model on the current feature set, using your defined `KFold` and scoring metric.
        * Store the resulting array of scores for each model.

### Step 3: Compare Model Performance and Conclude
* **Input:** The cross-validation scores for the three models.
* **Process:**
    1.  **Visualize Results:** Create a boxplot using `seaborn.boxplot` to compare the distribution of the Root Mean Squared Error (RMSE) scores for the three models (Baseline, PL, PI). This is the primary visualization for this analysis.
    2.  **Report Mean Scores:** Print a table summarizing the mean and standard deviation of the RMSE for each model.
    3.  **Synthesize and Conclude:** Write a final, conclusive markdown cell.
        * Formally compare the performance. Did the PL or PI models have a lower mean RMSE than the baseline?
        * Was the difference practically significant?
        * Based on the results, identify the winning model and feature set (e.g., "The model incorporating Persistence Landscape features (`X_pl`) achieved the lowest mean cross-validated RMSE, reducing error by 15% compared to the baseline model. The Persistence Image features also offered an improvement, but to a lesser extent. Therefore, the Persistence Landscape representation is the most effective for this task.")


## 4. Deliverables

1.  **Jupyter Notebook (`24_predict_sediment_yield.ipynb`)**: A fully executed notebook that serves as a complete report for the predictive modeling experiment. It must contain:
    * The data preparation, including the crucial log-transformation of the target variable.
    * The setup of the three distinct feature sets for the experiment.
    * The cross-validation workflow.
    * The final boxplot and summary table comparing the performance of the three models.
    * A clear, data-driven conclusion that formally identifies the best-performing model and discusses the value added by the different TLSP vectorizations.
2.  **(Optional but Recommended) Saved Model:** Save the final, best-performing model (trained on the full training dataset) to a file using `joblib.dump()` for use in the next interpretation step.

## 5. Quality Control Checklist

* [ ] The target variable (`sediment_yield`) has been correctly log-transformed.
* [ ] The three feature sets (`baseline`, `pl`, `pi`) are correctly defined and used.
* [ ] Model performance is evaluated using robust, k-fold cross-validation.
* [ ] The comparison of model performance is clearly visualized in a boxplot.
* [ ] The notebook concludes with an unambiguous, data-driven statement about the predictive power of the TLSP features and the relative effectiveness of PL vs. PI.
* [ ] The entire analysis is reproducible via the Jupyter Notebook.