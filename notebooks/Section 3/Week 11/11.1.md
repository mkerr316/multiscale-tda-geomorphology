# Protocol 11.1: Dimensionality Reduction for Visualization and Analysis

**Document Version:** 1.0
**Date:** September 27, 2025
**Associated Project Task:** 11.1 - Apply PCA/UMAP for dimensionality reduction.
**Corresponding Notebook:** `20_dimensionality_reduction.ipynb`

---

## 1. Objective

To apply two powerful dimensionality reduction techniques—Principal Component Analysis (PCA) and Uniform Manifold Approximation and Projection (UMAP)—to the high-dimensional covariate dataset. The goals are twofold:
1.  **To visualize** the intrinsic structure of the data and obtain a first, powerful visual test of the landscape characterization hypothesis ($H_1$).
2.  **To create** a set of lower-dimensional, orthogonal components that can be used as inputs for subsequent statistical tests, particularly MANOVA.

## 2. Rationale and Strategic Justification

This protocol is a profound act of **intellectual mastery**, representing the art and science of finding the hidden order within complexity. Your master dataset is a beautiful but high-dimensional object that cannot be directly perceived. Dimensionality reduction is the rigorous, mathematical technique that allows you to "see" its shadow in a lower-dimensional space, revealing the underlying structure that separates your geomorphic provinces.

The **harmony and beauty** are found in the final visualizations: elegant 2D scatter plots that transform a confusing, multi-column spreadsheet into a clear picture of landscape relationships. The decision to use both PCA (a linear method) and UMAP (a non-linear, topology-based method) is an "enthusiast-grade" choice that demonstrates a deep commitment to methodological rigor. UMAP, in particular, aligns perfectly with your project's core topological theme, seeking to preserve the essential shape of the data manifold.

The **tangible service** of this protocol is twofold: it provides the most compelling initial visual evidence for your entire project, and it prepares the data for formal statistical testing, ensuring that the assumptions of downstream models are met.

## 3. Implementation Protocol (Jupyter Notebook Workflow)

### Step 1: Prepare the Data
* **Input:** The final training dataset (`data/processed/final_training_dataset.csv`).
* **Process:**
    1.  Load the dataset into a `pandas` DataFrame.
    2.  Separate the data into two components:
        * `X`: A DataFrame containing only the predictor variables (all continuous Tier 1, 2, and 3 covariates).
        * `y`: A pandas Series containing the grouping variable (the `province_ID`).
    3.  **Crucially, scale the predictor data.** Use `sklearn.preprocessing.StandardScaler` to transform `X` into a new array `X_scaled` where each feature has a mean of 0 and a standard deviation of 1. This is a mandatory prerequisite for both PCA and UMAP to work correctly.

### Step 2: Apply Principal Component Analysis (PCA)
* **Input:** The scaled predictor data, `X_scaled`.
* **Process:**
    1.  Instantiate `sklearn.decomposition.PCA`. A good practice is to set `n_components=0.95` to automatically retain the number of components needed to explain 95% of the total variance.
    2.  Fit the PCA model to `X_scaled` and transform the data into its principal components.
    3.  **Analyze the Scree Plot:** Visualize the `explained_variance_ratio_` attribute of the fitted PCA object as a bar chart. This plot is essential for understanding how many components are needed to capture the majority of the information in the data.
    4.  **Visualize the Results:** Create a scatter plot of the first two principal components (PC1 vs. PC2). Color the points in the scatter plot according to their geomorphic province (`y`).

### Step 3: Apply Uniform Manifold Approximation and Projection (UMAP)
* **Input:** The scaled predictor data, `X_scaled`.
* **Process:**
    1.  Instantiate `umap.UMAP` with `n_components=2`. Other key parameters, such as `n_neighbors` and `min_dist`, can be tuned but the defaults are often a good starting point.
    2.  Fit the UMAP model to `X_scaled` and transform the data.
    3.  **Visualize the Results:** Create a scatter plot of the two UMAP components, again coloring the points by their geomorphic province.



### Step 4: Compare, Interpret, and Conclude
* **Process:**
    1.  Display the PCA and UMAP plots side-by-side.
    2.  Write a detailed markdown cell that interprets the results:
        * Does one method show a clearer visual separation between the geomorphic provinces than the other? (UMAP often will, as it is better at preserving cluster structure).
        * This is the first strong piece of evidence supporting or refuting $H_1$.
    3.  Conclude which components will be carried forward for formal statistical testing. The principal components from PCA are typically used for MANOVA because they are mathematically guaranteed to be orthogonal (uncorrelated), which is a desirable property for the test.
    4.  Create a final DataFrame that joins the principal component scores back to the original sample data (including sample IDs and province IDs).

## 4. Deliverables

1.  **Final Dataset with Components:** A CSV file (`data/processed/training_data_with_components.csv`) containing the original sample information plus the new principal component scores for each point.
2.  **Jupyter Notebook (`20_dimensionality_reduction.ipynb`)**: A fully executed notebook that serves as a complete report on the dimensionality reduction analysis. It must include:
    * The data scaling step.
    * The PCA scree plot and the PC1 vs. PC2 visualization.
    * The UMAP visualization.
    * A detailed written interpretation and comparison of the PCA and UMAP results, and a clear justification for the components selected for the next stage of analysis.

## 5. Quality Control Checklist

* [ ] The predictor data was correctly scaled *before* applying PCA and UMAP.
* [ ] A scree plot for the PCA was generated and interpreted.
* [ ] Both PCA and UMAP visualizations are clearly labeled and colored by geomorphic province.
* [ ] The notebook contains a thoughtful, written comparison of the two methods' results.
* [ ] The final output CSV file correctly merges the new component scores with the original sample identifiers.
* [ ] The entire process is documented and reproducible via the notebook.