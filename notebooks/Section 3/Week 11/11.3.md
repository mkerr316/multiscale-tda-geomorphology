# Protocol 11.3: Supervised Classification of Geomorphic Provinces

**Document Version:** 1.0
**Date:** September 27, 2025
**Associated Project Task:** 11.3 - Train and validate a Random Forest classifier.
**Corresponding Notebook:** `22_classify_geomorphic_provinces.ipynb`

---

## 1. Objective

To train, tune, and rigorously validate a Random Forest machine learning model to classify sample points into their respective geomorphic provinces. The primary goals are:
1.  To obtain a quantitative measure of the classification accuracy achievable using the full suite of geomorphometric and topological covariates.
2.  To determine the relative importance of each covariate, identifying which landscape metrics are the most powerful discriminators of geomorphic setting.
3.  To explicitly compare the performance of a model built with novel TLSP features against a baseline model using only traditional metrics, as outlined in the project proposal [cite: Geomorphometry Methods and Metrics Discussion.md].

## 2. Rationale and Strategic Justification

This protocol is where your project's predictive power is first unleashed. It is an act of **intellectual mastery**, moving beyond statistical testing to build a functional, predictive machine. Your choice of a Random Forest classifier is an "enthusiast-grade" decisionâ€”it is a powerful, non-linear model that is robust to the complex interactions and non-normal distributions common in environmental data, and it provides a beautiful, built-in method for assessing feature importance.

The **harmony and beauty** of this protocol are revealed in the final outputs: an elegant confusion matrix that clearly visualizes the model's predictive success, and an insightful feature importance plot that creates a "power ranking" of your landscape metrics. This provides a **tangible service** by delivering a clear, quantitative answer to your first hypothesis ($H_1$) from a new, predictive perspective. By building and comparing two models (baseline vs. topological), you are performing a rigorous scientific experiment that will definitively demonstrate the added value of your novel TLSP covariates.

## 3. Implementation Protocol (Jupyter Notebook Workflow)

### Step 1: Feature Engineering and Data Splitting
* **Input:** The final training dataset (`data/processed/final_training_dataset.csv`).
* **Process:**
    1.  Load the dataset into a `pandas` DataFrame.
    2.  Define the target variable: `y = df['province_ID']`.
    3.  **Create Two Predictor Sets (Crucial Step):**
        * **`X_baseline`**: A DataFrame containing only the **Tier 1 and Tier 2** covariates (DEM, slope, relief, roughness, TWI, SPI).
        * **`X_full`**: A DataFrame containing **all covariates**, including the Tier 3 TLSP and Geomorphon features.
    4.  For this classification task, you do not need a separate train-test split, as we will use k-fold cross-validation for robust evaluation.

### Step 2: Model Training, Tuning, and Cross-Validation
This step combines hyperparameter tuning and model validation into a single, efficient workflow.

* **Input:** `X_baseline`, `X_full`, and `y`.
* **Process:**
    1.  **Define the Model:** Instantiate a `RandomForestClassifier` from `sklearn.ensemble`.
    2.  **Define Hyperparameter Grid:** Specify a dictionary of hyperparameters to tune, such as `n_estimators` (number of trees), `max_depth` (depth of trees), and `min_samples_leaf`.
    3.  **Set up Cross-Validation:** Use `sklearn.model_selection.GridSearchCV` to automate the process. This will systematically test all hyperparameter combinations using stratified k-fold cross-validation (e.g., `cv=5`) to find the best model.
    4.  **Execute for Both Feature Sets:**
        * Run `GridSearchCV` on the `X_baseline` data.
        * Run `GridSearchCV` on the `X_full` data.

### Step 3: Model Evaluation and Comparison
* **Input:** The two fitted `GridSearchCV` objects.
* **Process:**
    1.  **Extract Best Scores:** For each model, report the best cross-validated accuracy score (`best_score_`).
    2.  **Generate Predictions:** Use the best-tuned model from the `X_full` dataset to generate cross-validated predictions for the entire training set using `sklearn.model_selection.cross_val_predict`.
    3.  **Create Confusion Matrix:** Use the cross-validated predictions to compute a confusion matrix using `sklearn.metrics.confusion_matrix`. Visualize this matrix as a heatmap with clear labels for true and predicted classes.
    4.  **Create Classification Report:** Print a full classification report using `sklearn.metrics.classification_report`, which includes precision, recall, and F1-score for each province.


### Step 4: Analyze Feature Importance
* **Input:** The best-tuned model trained on the `X_full` dataset.
* **Process:**
    1.  Extract the feature importances from the trained classifier (`best_estimator_.feature_importances_`).
    2.  Create a `pandas` Series or DataFrame that maps these importance scores to their corresponding covariate names.
    3.  Sort the importances in descending order.
    4.  Visualize the results as a horizontal bar chart, clearly showing which landscape metrics were most influential in the classification.

### Step 5: Synthesize and Conclude
* **Process:**
    1.  Write a final, conclusive markdown cell in the notebook.
    2.  The conclusion must compare the performance of the baseline model vs. the full model. Did adding the TLSP and Geomorphon features improve the cross-validated accuracy?
    3.  Discuss the confusion matrix. Are the provinces easily separable, or is there confusion between them?
    4.  Interpret the feature importance plot. Which covariates were most important? Did the novel TLSP features rank highly, providing direct evidence of their utility?

## 4. Deliverables

1.  **Jupyter Notebook (`22_classify_geomorphic_provinces.ipynb`)**: A fully executed notebook that serves as a complete report for the classification analysis. It must contain:
    * The setup of the baseline and full feature sets.
    * The `GridSearchCV` workflow for tuning and cross-validating both models.
    * A clear comparison of the final accuracy scores.
    * The confusion matrix and classification report for the best model.
    * The feature importance bar chart and its interpretation.
    * A final written conclusion synthesizing all findings.

## 5. Quality Control Checklist

* [ ] Two distinct feature sets (baseline and full) were created and tested.
* [ ] Hyperparameter tuning was performed using robust, stratified k-fold cross-validation (`GridSearchCV`).
* [ ] The final model performance is reported using cross-validated metrics, not just a single train-test split.
* [ ] The confusion matrix and feature importance plots are clearly visualized and interpreted.
* [ ] The notebook concludes with a clear, data-driven comparison of the two models and an assessment of the value added by the Tier 3 covariates.
* [ ] The entire analysis is reproducible via the Jupyter Notebook.