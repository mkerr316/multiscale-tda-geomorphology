# Protocol 9.1: Assembling the Final Analysis-Ready Datasets

**Document Version:** 1.0
**Date:** September 27, 2025
**Associated Project Task:** 9.1 - Extract values from all generated covariate rasters at the final sample locations.
**Corresponding Notebook:** `18_extract_sample_data.ipynb`

---

## 1. Objective

To extract the values from all generated covariate rasters (Tiers 1, 2, and 3, including all TLSP variants) at the final, doubly balanced sample locations. The outcome will be the creation of the master, analysis-ready dataset—a clean, tabular file where each row represents a sample point and each column represents a measured variable, perfectly prepared for the subsequent modeling and analysis phase.

## 2. Rationale and Strategic Justification

This protocol is the final act of **creative world-building** in your project's data engineering phase. It is a profound moment of synthesis where the two most beautiful and complex creations of your project—the perfectly balanced sample design and the rich, multiscale covariate stack—are harmoniously united into a single, elegant data structure.

This is an act of **intellectual mastery** over the full data pipeline, demonstrating the ability to manage and integrate dozens of complex spatial layers with precision. The **tangible service** provided by this task is immense and critical: it delivers the clean, organized, and final dataset upon which all subsequent scientific inquiry will rest. Without this master table, the project is just a collection of disparate files; with it, you have a powerful, integrated scientific instrument ready for discovery.

## 3. Implementation Protocol (Jupyter Notebook Workflow)

### Step 1: Load Final Sample Points and Covariate List
* **Input:**
    * The final sample points GeoPackage (`data/processed/sampling/final_sample_points.gpkg`).
    * The directory containing all unstandardized covariate rasters (`data/processed/covariates/unstandardized/`).
* **Process:**
    1.  Load the sample points into a `geopandas` GeoDataFrame.
    2.  Use Python's `glob` or `os` module to get a complete list of all covariate raster file paths. It is crucial to use the **unstandardized** versions to preserve the original, interpretable units of each variable.

### Step 2: Implement the Data Extraction Function
* **Input:** The sample points GeoDataFrame and the list of raster paths.
* **Process:**
    1.  Create a new `pandas` DataFrame, initialized with the data from the sample points GeoDataFrame (e.g., coordinates, stratum ID).
    2.  Write a loop that iterates through each raster file path.
    3.  Inside the loop:
        * Open the raster using `rioxarray`.
        * Use the `.sel()` method of the `rioxarray` DataArray to select the values at the `x` and `y` coordinates of your sample points. This is an efficient, vectorized method for point sampling.
        * Extract the resulting values into a `numpy` array.
        * Add this array as a new column to your `pandas` DataFrame. Use the filename as the column name for clarity.


### Step 3: Data Extraction for the Hold-Out Validation Area
To maintain the integrity of your validation process, a separate (but identical) extraction must be performed for the hold-out area.

* **Process:**
    1.  Draw a new, simple, spatially balanced sample of points (of the same size `n`) within your finalized `validation_area.gpkg`.
    2.  Repeat the entire extraction process from Step 2 using these new validation points.
    3.  Save the result as a separate DataFrame. This strict separation of training and validation data is critical for an honest assessment of model performance.

### Step 4: Final Cleaning and Export
* **Input:** The two newly created DataFrames (training and validation).
* **Process:**
    1.  For each DataFrame, perform a final quality check. Use `.isnull().sum()` to confirm there are no missing values. Handle any missing data that may have resulted from points falling on `nodata` edges (e.g., by removing the point or using a nearest-neighbor fill).
    2.  Ensure column names are clean and descriptive.
    3.  Export both DataFrames to CSV format.

## 4. Deliverables

1.  **Final Training Dataset:** A CSV file (`data/processed/final_training_dataset.csv`) containing the complete, analysis-ready data for the primary study areas.
2.  **Final Validation Dataset:** A CSV file (`data/processed/final_validation_dataset.csv`) containing the complete, analysis-ready data for the independent hold-out validation area.
3.  **Jupyter Notebook (`18_extract_sample_data.ipynb`)**: A fully executed notebook documenting the entire extraction process for both training and validation sets, including the final quality control checks.

## 5. Quality Control Checklist

* [ ] The number of rows in the final training dataset matches the number of points in the final sample draw.
* [ ] The number of columns matches the number of sample attributes plus the total number of generated covariate rasters.
* [ ] A check for `NaN` or null values confirms the datasets are complete and clean.
* [ ] The training and validation datasets are saved as separate files and were generated from spatially independent sample points.
* [ ] The entire workflow is documented and reproducible via the Jupyter Notebook.