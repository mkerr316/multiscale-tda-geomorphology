# Protocol 8.1: Productionizing and Executing the Parallel TDA Pipeline

**Document Version:** 1.0
**Date:** September 27, 2025
**Associated Project Task:** 8.1 - Wrap the validated TDA function using the optimal parallel backend, incorporating halo regions & Generate the final suite of TLSP raster layers.
**Corresponding Notebook:** `16_generate_tlsp_rasters.ipynb`

---

## 1. Objective

To refactor the validated core TDA function into a production-ready, parallelized pipeline using the Dask-CPU backend. This involves wrapping the function with Dask's `map_overlap` to correctly handle boundary conditions ("halo regions") and executing this pipeline on the full study area DEMs to generate the final suite of novel TLSP covariate rasters.

## 2. Rationale and Strategic Justification

This protocol is the grand synthesis of your entire methodological framework. It is the ultimate expression of **Structured Idealism**: the idealistic vision of a new, shape-based language for geomorphology is made real through the rigorous, logical structure of a parallel computing pipeline.

This act represents **intellectual mastery** over the full stack of computational geoscience, from abstract topological theory to the concrete engineering challenges of scalable computing. The **harmony and beauty** of this protocol are found in the elegance of the `dask.array.map_overlap` functionâ€”a single, powerful command that seamlessly orchestrates the complex dance of windowing, parallel execution, and halo management to produce a perfect, artifact-free result.

The **tangible service** this protocol provides is the creation of the project's most valuable and unique data assets: the final TLSP rasters. These novel covariates are the key deliverable that will either prove or disprove your core hypotheses, making the successful execution of this workflow the capstone of your feature engineering phase.

## 3. Implementation Protocol (Jupyter Notebook Workflow)

### Step 1: Refactor and Productionize the TDA Wrapper Function
This step moves your validated code into its final, reusable form.

* **Process:**
    1.  Open your utility script, `src/tda_utils.py`.
    2.  Create a new, high-level function: `def generate_tlsp_raster(dem_array, window_size, homology_dim, invert=False):`
    3.  This function will orchestrate the entire parallel computation.
    4.  **Calculate Overlap Depth:** Inside the function, the first step is to calculate the necessary halo or overlap region. This is determined by the analysis window size: `depth = window_size // 2`. This ensures that every sliding window has access to a full neighborhood of data, preventing edge artifacts between Dask chunks.
    5.  **Wrap with Dask:** Use the `dask.array.map_overlap` function to apply your core `calculate_local_topology` function to the input `dem_array`.
        * `depth=depth`: The calculated halo size.
        * `boundary='reflect'`: A robust method for handling the edges of the entire DEM.
        * `trim=True`: This is a critical parameter that tells Dask to automatically trim the halo regions from the output chunks, ensuring the final raster has the exact same dimensions as the input DEM.
    6.  The function should return the resulting Dask array, which represents the full TLSP raster *before* computation.

### Step 2: Execute the Pipeline to Generate TLSP Rasters
This is the main execution phase, which will be performed in the notebook.

* **Input:**
    * The primary conditioned DEM (`data/processed/dem_filled.tif`).
    * The finalized Dask wrapper function from `src/tda_utils.py`.
* **Process:**
    1.  Initialize your dynamic Dask client: `from src.dask_setup import get_dask_client; client = get_dask_client()`.
    2.  Load the full DEM for your study areas as a Dask-backed `rioxarray` object.
    3.  **Generate H1 (Pits/Basins) TLSP:**
        * Call your new `generate_tlsp_raster` function with the DEM, `homology_dim=1`, and `invert=False`.
        * Trigger the computation by calling `.compute()` on the result.
        * Save the output NumPy array as a properly georeferenced GeoTIFF using `rioxarray`. Name it descriptively, e.g., `tlsp_h1_pits.tif`.
    4.  **Generate H0 (Peaks/Ridges) TLSP:**
        * Call `generate_tlsp_raster` again, but this time with the *inverted* DEM (`-dem_array`), `homology_dim=0`, and `invert=True` (or handle inversion within the wrapper).
        * Trigger the computation and save the result as `tlsp_h0_peaks.tif`.
    5.  Monitor the Dask dashboard during execution to observe the parallel performance.



## 4. Deliverables

1.  **Production-Ready TDA Script (`src/tda_utils.py`)**: The updated utility script containing the final, production-grade `generate_tlsp_raster` function that wraps the core TDA logic in the Dask parallel backend.
2.  **Final TLSP Covariate Rasters:** The primary novel data products of the project, stored in `data/processed/covariates/unstandardized/`:
    * `tlsp_h1_pits.tif`: A raster quantifying the topological complexity of depressions.
    * `tlsp_h0_peaks.tif`: A raster quantifying the topological complexity of peaks and ridges.
3.  **Jupyter Notebook (`16_generate_tlsp_rasters.ipynb`)**: A fully executed notebook that:
    * Imports the production-ready function.
    * Executes the function on the full study area DEMs to generate the two TLSP layers.
    * Saves the final GeoTIFF files.
    * Includes visualizations of the final TLSP rasters.

## 5. Quality Control Checklist

* [ ] The production function correctly calculates and applies the halo `depth` for `map_overlap`.
* [ ] The Dask execution runs to completion on the full DEMs without memory errors.
* [ ] The final output TLSP rasters have the exact same dimensions, CRS, and alignment as the input DEM, with no visible edge artifacts between chunks.
* [ ] The values in the output rasters are plausible and show clear geomorphic patterns.
* [ ] The entire generation process is documented and reproducible via the notebook.