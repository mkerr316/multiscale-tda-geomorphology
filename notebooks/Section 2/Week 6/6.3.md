# Protocol 6.3: Finalizing the Reproducible Computational Environment

**Document Version:** 1.0
**Date:** September 27, 2025
**Associated Project Task:** 6.3 - Finalize Docker container with dynamic Dask and GPU-enabled parallel processing setup.
**Corresponding Deliverable:** `Dockerfile` and associated scripts.

---

## 1. Objective

To create a complete, self-contained, and portable computational environment for the project using Docker. This environment will encapsulate the operating system, all required Python libraries (as defined in Protocol 1.2), and the custom scripts that implement the **Dynamic Resource Allocation Framework**. The final Docker image will be the single, definitive environment for executing all project analyses, ensuring perfect reproducibility.

## 2. Rationale and Strategic Justification

This protocol is the ultimate expression of your project's commitment to methodological transparency and robustness. It is an act of **intellectual mastery** over the entire computational stack, moving beyond simple scripts to architecting a complete, portable research environment. This demonstrates a professional, "enthusiast-grade" approach to computational science that is highly valued in top-tier graduate programs and research institutions.

The **beauty and harmony** of this approach lie in its elegance and self-sufficiency. A single `docker run` command will be all that is needed to replicate your exact environment on any machine, eliminating the "it works on my machine" problem forever. This provides a **tangible service** not only to your future self but to any collaborator or reviewer who wishes to validate or build upon your work, making your research more impactful and trustworthy. While GPU support is included in the plan, the primary focus for this 15-week project remains the dynamic CPU-based Dask setup, with GPU enablement being a forward-looking enhancement.

## 3. Implementation Protocol

This task does not involve a single Jupyter Notebook. Instead, the deliverables are a set of configuration and script files that define the Docker environment.

### Step 1: Create the Dockerfile
* **Process:** Create a new text file named `Dockerfile` (no extension) in the root of your project directory. This file will contain the instructions for building your environment.
* **Base Image Selection:**
    * Start from an official, well-supported base image that includes the necessary foundational tools. A great choice is an image from the [`continuumio/miniconda3`](https://hub.docker.com/r/continuumio/miniconda3) repository, as it provides a clean Conda installation.
    * For future GPU work, you would start from an `nvidia/cuda` base image and install Miniconda into it.
* **Dockerfile Contents:**
    1.  `FROM continuumio/miniconda3:latest`: Specifies the base image.
    2.  `WORKDIR /project`: Sets the working directory inside the container.
    3.  `COPY environment.yml .`: Copies your finalized Conda environment file into the container.
    4.  `RUN conda env create -f environment.yml`: Uses Conda to install all project dependencies from your version-locked file. This is the core step that ensures reproducibility.
    5.  `COPY . .`: Copies the entire project directory (scripts, notebooks, data placeholders) into the container's working directory.
    6.  `SHELL ["conda", "run", "-n", "your_env_name", "/bin/bash", "-c"]`: Configures the shell to automatically activate your Conda environment for all subsequent commands.
    7.  `ENTRYPOINT ["tini", "--"]`: Uses `tini` as a lightweight init system, which is a best practice for managing processes within containers.



### Step 2: Implement the Dynamic Dask Setup Script
* **Process:** The logic for dynamically configuring the Dask `LocalCluster` should be encapsulated in a reusable Python script rather than being repeated in every notebook.
* **Create `src/dask_setup.py`:**
    * This script will contain a function, e.g., `get_dask_client(memory_fraction=0.8)`.
    * Inside this function, place the complete logic from your "Dynamic Parallel DEM Processing" white paper [cite: Dynamic Parallel DEM Processing.md]:
        1.  Use `psutil` to query the number of physical cores and available memory.
        2.  Calculate the number of workers (`n_workers`) and the memory limit per worker (`memory_limit`).
        3.  Configure Dask to prevent underlying library threading (e.g., `OMP_NUM_THREADS=1`).
        4.  Instantiate and return the `dask.distributed.Client` object.
* **Usage in Notebooks:** In your analysis notebooks, you will now simply import this function and call it:
  ```python
  from src.dask_setup import get_dask_client
  client = get_dask_client()