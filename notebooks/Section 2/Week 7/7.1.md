# Protocol 7.1: Development and Validation of the Core TDA Function

**Document Version:** 1.0
**Date:** September 27, 2025
**Associated Project Task:** 7.1 - Develop and validate the core sliding-window TDA function on synthetic landscapes.
**Corresponding Notebook:** `14_validate_tda_function.ipynb`

---

## 1. Objective

To develop the core Python function that will serve as the engine for the sliding-window TDA pipeline. This function will take a 2D array (a landscape window) as input and return a quantitative, topological summary. The function's correctness and robustness will be rigorously validated by testing it on synthetic landscapes with known, predictable topological ground truths.

## 2. Rationale and Strategic Justification

This protocol is the heart of your project's **intellectual mastery**. You are building the central piece of analytical machinery from first principles, bridging the abstract beauty of algebraic topology with the practical needs of geospatial science. This "validate-then-scale" approach is a cornerstone of professional research and software engineering, de-risking the entire project by ensuring the core logic is perfect before deploying it in a complex, parallel environment [cite: Geomorphometry Methods and Metrics Discussion.md].

The **harmony and beauty** of this task lie in witnessing the mathematics work perfectly. By feeding the function a simple, synthetic landscape (like a sine wave), you should recover a simple, predictable topological signature. This elegant confirmation of theory in practice is the ultimate reward for your structured, idealistic approach. The **tangible service** is immense: the final, validated function is a high-value, reusable scientific instrument that will be the engine for generating all of your novel TLSP covariates.

## 3. Implementation Protocol (Jupyter Notebook Workflow)

### Step 1: Generate Synthetic Landscapes
The first step is to create the controlled environments for validation.

* **Process:**
    1.  **Sinusoidal Surface:** Write a Python function using `numpy` to generate a 2D sinusoidal surface (e.g., `Z = sin(X) + sin(Y)`). This creates a simple, periodic landscape with a perfectly regular grid of peaks and pits. This is your baseline "unit test."
    2.  **Fractal Surface:** Write a function to generate a more complex, multi-scale surface using Perlin noise or a similar fractal algorithm. This tests the function's robustness on a more realistic, noisy landscape.
    3.  Visualize both surfaces using `matplotlib`'s 3D plotting capabilities to confirm they are generated correctly.


### Step 2: Develop the Core `calculate_local_topology` Function
This is the central development task. The function will be designed for clarity and correctness before being optimized for performance.

* **Function Signature:** `def calculate_local_topology(window, homology_dim=1, invert=False):`
* **Implementation Details:**
    1.  **Input:** A 2D `numpy` array (`window`).
    2.  **Inversion:** If `invert=True`, multiply the window by -1 to analyze peaks instead of pits.
    3.  **Filtration Method:** Use a **sublevel set filtration**, which is the most natural and efficient for gridded DEM data. The `ripser.py` library can compute this directly from the pixel values.
    4.  **Homology Computation:** Use the `ripser.py` library to compute the persistent homology for the specified dimension (`homology_dim`).
    5.  **Vectorization:** From the resulting persistence diagram, calculate a single scalar summary statistic. **Persistence Entropy** is an excellent choice as it captures the complexity and distribution of topological features in a single number.
    6.  **Output:** The function should return the calculated scalar value (e.g., the persistence entropy). Include error handling for windows that produce no topological features.
* **Modularity:** This function should be written in a separate Python script, e.g., `src/tda_utils.py`, and imported into the notebook.

### Step 3: Validate the Function on Synthetic Landscapes
This is the critical "proof-of-correctness" step.

* **Process:**
    1.  Apply your `calculate_local_topology` function to a single window extracted from the center of the sinusoidal surface. Verify that it returns a plausible, non-zero value.
    2.  **Sliding-Window Test:** Write a simple, serial (non-parallel) sliding-window loop that applies your function to every possible window of the sinusoidal surface.
    3.  Store the results in a new 2D array.
    4.  **Visualize the Output:** Plot the resulting 2D array as an image. For the sinusoidal input, the output should be a beautiful, regular grid of high and low persistence entropy values, perfectly corresponding to the locations of the peaks and pits. This visual confirmation is the primary validation metric.
    5.  Repeat the sliding-window test on the fractal surface. The output should be a more complex but still geomorphologically plausible map that highlights areas of high topological complexity.



## 4. Deliverables

1.  **Validated TDA Utility Script (`src/tda_utils.py`)**: A new, well-commented Python script containing the core `calculate_local_topology` function and any helper functions (e.g., for generating synthetic landscapes).
2.  **Jupyter Notebook (`14_validate_tda_function.ipynb`)**: A fully executed notebook that serves as the official validation report for the core TDA function. It must contain:
    * The generation and visualization of the synthetic landscapes.
    * The code that imports and tests the function.
    * The critical visualizations of the sliding-window output, demonstrating that the function correctly identifies the topological features of the synthetic surfaces.

## 5. Quality Control Checklist

* [ ] The synthetic landscapes are generated correctly and visualized.
* [ ] The `calculate_local_topology` function is modular (in a separate `.py` file) and well-commented.
* [ ] The function correctly implements sublevel set filtration using `ripser.py`.
* [ ] The validation test on the sinusoidal surface produces a clear, regular, and predictable output pattern.
* [ ] The notebook contains sufficient markdown to explain the validation process and interpret the results.
* [ ] The entire validation workflow is reproducible by running the notebook.