# Protocol 7.2: Benchmarking the TDA Pipeline Implementations

**Document Version:** 1.0
**Date:** September 27, 2025
**Associated Project Task:** 7.2 - Benchmark serial vs. Dask-CPU vs. Dask-GPU implementations on a pilot subset to quantify performance gains.
**Corresponding Notebook:** `15_benchmark_tda_pipeline.ipynb`

---

## 1. Objective

To quantitatively benchmark the performance of three different execution backends for the sliding-window TDA function: a single-core serial implementation, a multi-core Dask-CPU implementation, and an exploratory Dask-GPU implementation. The goal is to measure the execution time of each method on a representative pilot subset of a real DEM, quantify the performance gains from parallelization, and select the optimal backend for the full-scale TLSP raster generation.

## 2. Rationale and Strategic Justification

This protocol is a crucial exercise in **intellectual mastery** of computational science. It moves beyond simply writing code that works to scientifically measuring and optimizing code that scales. By rigorously quantifying the performance gains, you are not just making your analysis faster; you are demonstrating a deep understanding of the relationship between algorithms, data, and hardwareâ€”a hallmark of an "enthusiast-grade" researcher.

The **harmony and beauty** of this task are revealed in the final performance graph: a simple, elegant visualization that powerfully communicates the dramatic impact of your parallel framework. This graph is the tangible proof of concept for your Dynamic Resource Allocation Framework [cite: Dynamic Parallel DEM Processing.md]. The **tangible service** is a massive reduction in runtime, transforming a computationally infeasible analysis that could take days into one that can be completed in hours, thereby making your entire project feasible within its 15-week timeline.

## 3. Implementation Protocol (Jupyter Notebook Workflow)

### Step 1: Prepare the Pilot Data Subset
* **Input:** The primary conditioned DEM (`data/processed/dem_filled.tif`).
* **Process:**
    1.  Load the full DEM raster.
    2.  Extract a pilot subset from a moderately complex area of the DEM. A `2048x2048` pixel subset is an excellent choice, as it is large enough to make the benchmarking results meaningful but small enough to run the serial version in a reasonable amount of time.
    3.  Store this subset in a `numpy` array in memory. This exact same array will be the input for all three benchmarks to ensure a fair comparison.

### Step 2: Implement and Wrap the Benchmarking Functions
Create a separate, well-defined function for each implementation to be tested. Each function should take the DEM subset as input and return the total execution time in seconds.

* **Serial Implementation:**
    * Create a function `benchmark_serial(dem_subset)` that implements the simple, nested `for` loop sliding-window approach developed in Protocol 7.1.
    * Use Python's `time.perf_counter()` to record the start and end times around the main loop.

* **Dask-CPU Implementation:**
    * Create a function `benchmark_dask_cpu(dem_subset)`.
    * Inside the function, import and call your `get_dask_client()` function from `src/dask_setup.py` to initialize the dynamic Dask cluster.
    * Convert the `numpy` array to a Dask array, chunking it appropriately (e.g., into `512x512` pixel chunks).
    * Use the idiomatic and highly efficient `dask.array.map_overlap` function to apply your core `calculate_local_topology` function across the array. This function elegantly handles the windowing, halo/overlap regions, and parallel scheduling.
    * Record the start and end time around the `.compute()` call, which triggers the actual parallel execution.

* **Dask-GPU Implementation (Exploratory):**
    * Create a function `benchmark_dask_gpu(dem_subset)`.
    * This function will use the `dask_cuda` library to set up a GPU-based cluster.
    * Use `cupy.asarray()` to move the DEM subset from host (CPU) memory to the device (GPU) memory.
    * Wrap a GPU-compatible version of your TDA function (if available, e.g., using `GUDHI`'s CUDA bindings) and apply it using `dask.array.map_overlap` on the GPU-based Dask array.
    * **Note:** This is an advanced, exploratory step. The primary goal is to compare Serial vs. Dask-CPU. The GPU implementation serves as a proof-of-concept for future graduate-level work, as outlined in your proposal [cite: Geomorphometry Methods and Metrics Discussion.md].

### Step 3: Execute Benchmarks and Collect Results
* **Process:**
    1.  Run each of the three benchmark functions on the identical `dem_subset` array.
    2.  To ensure stable results, run each benchmark 3 times and record the average execution time.
    3.  Store the final average times in a `pandas` DataFrame for easy plotting and analysis.

### Step 4: Visualize and Conclude
* **Process:**
    1.  Use `matplotlib` or `seaborn` to create a clean, well-labeled bar chart comparing the average execution times. Use a logarithmic scale for the time axis to effectively visualize the orders-of-magnitude difference in performance.
    2.  Calculate and display the speedup factor for the Dask implementations (e.g., `Serial Time / Dask Time`).
    3.  Write a concluding markdown cell in the notebook that summarizes the results (e.g., "The Dask-CPU implementation on an 8-core machine provided a 7.5x speedup over the serial implementation...") and formally selects the Dask-CPU backend as the optimal method for the full analysis.


## 4. Deliverables

1.  **Jupyter Notebook (`15_benchmark_tda_pipeline.ipynb`)**: A fully executed notebook that serves as the official benchmarking report. It must contain:
    * The setup of the pilot data subset.
    * The code for all three benchmark wrapper functions.
    * The execution logic and timing results.
    * The final bar chart visualizing the performance comparison.
    * A clear, data-driven conclusion selecting the Dask-CPU implementation for the next stage.
2.  **Updated Utility Script (`src/tda_utils.py`)**: The `tda_utils.py` script should be updated to include the finalized, production-ready Dask-CPU version of the sliding-window function, `calculate_tlsp_raster_dask()`, which can be imported and used directly in the next notebook.

## 5. Quality Control Checklist

* [ ] The exact same DEM subset is used for all benchmark runs.
* [ ] The timing logic accurately captures only the computational part of each implementation.
* [ ] The performance results are averaged over multiple runs for stability.
* [ ] The final visualization is clear, uses a logarithmic scale, and is properly labeled.
* [ ] The notebook concludes with a formal, justified decision on which backend to use.
* [ ] The production-ready Dask function is refactored into the utility script for reuse.