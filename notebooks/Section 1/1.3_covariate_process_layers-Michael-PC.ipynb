{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4201e35729ddf493",
   "metadata": {},
   "source": [
    "# Define & Justify Covariate & Process Layers\n",
    "Identify and acquire wall-to-wall raster data for:\n",
    "- Balancing Covariates: (e.g., slope, relief, curvature). Justify selection based on potential variance reduction and geomorphic interpretability.\n",
    "- Process-Based Metrics: (e.g., Topographic Wetness Index, Stream Power Index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c760265d02971fb",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# === 1. Dynamic Resource Allocation & Dask Setup with Auto-Installation ===\n",
    "\n",
    "# --- Auto-Install Function (define first) ---\n",
    "def install_package(package_name, conda_name=None, channel=\"conda-forge\"):\n",
    "    \"\"\"Install a package using micromamba.\"\"\"\n",
    "    try:\n",
    "        conda_package = conda_name if conda_name else package_name\n",
    "        print(f\"Installing {conda_package} from {channel}...\")\n",
    "        subprocess.check_call([\n",
    "            \"micromamba\", \"install\", \"-n\", \"app\",\n",
    "            \"-c\", channel, conda_package, \"-y\"\n",
    "        ], env=os.environ.copy())\n",
    "        print(f\"Successfully installed {conda_package}\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to install {conda_package}: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- Core Libraries with Auto-Install ---\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "# Standard scientific libraries (should already be installed)\n",
    "import numpy as np\n",
    "import psutil\n",
    "import math\n",
    "\n",
    "# Dask\n",
    "try:\n",
    "    import dask\n",
    "    import dask.array as da\n",
    "    from dask.diagnostics import ProgressBar\n",
    "    from dask.distributed import Client, LocalCluster\n",
    "    from dask import delayed\n",
    "    print(\"Dask found.\")\n",
    "except ImportError:\n",
    "    print(\"Dask not found. Installing...\")\n",
    "    if install_package(\"dask\") and install_package(\"distributed\"):\n",
    "        import dask\n",
    "        import dask.array as da\n",
    "        from dask.diagnostics import ProgressBar\n",
    "        from dask.distributed import Client, LocalCluster\n",
    "        from dask import delayed\n",
    "        print(\"Dask successfully installed and imported.\")\n",
    "    else:\n",
    "        raise ImportError(\"Could not install Dask. Please install manually.\")\n",
    "\n",
    "# Rasterio\n",
    "try:\n",
    "    import rasterio\n",
    "    from rasterio.windows import Window\n",
    "    print(\"Rasterio found.\")\n",
    "except ImportError:\n",
    "    print(\"Rasterio not found. Installing...\")\n",
    "    if install_package(\"rasterio\"):\n",
    "        import rasterio\n",
    "        from rasterio.windows import Window\n",
    "        print(\"Rasterio successfully installed and imported.\")\n",
    "    else:\n",
    "        raise ImportError(\"Could not install Rasterio. Please install manually.\")\n",
    "\n",
    "# Rioxarray\n",
    "try:\n",
    "    import rioxarray as rxr\n",
    "    print(\"Rioxarray found.\")\n",
    "except ImportError:\n",
    "    print(\"Rioxarray not found. Installing...\")\n",
    "    if install_package(\"rioxarray\"):\n",
    "        import rioxarray as rxr\n",
    "        print(\"Rioxarray successfully installed and imported.\")\n",
    "    else:\n",
    "        raise ImportError(\"Could not install Rioxarray. Please install manually.\")\n",
    "\n",
    "# Xarray\n",
    "try:\n",
    "    import xarray as xr\n",
    "    print(\"Xarray found.\")\n",
    "except ImportError:\n",
    "    print(\"Xarray not found. Installing...\")\n",
    "    if install_package(\"xarray\"):\n",
    "        import xarray as xr\n",
    "        print(\"Xarray successfully installed and imported.\")\n",
    "    else:\n",
    "        raise ImportError(\"Could not install Xarray. Please install manually.\")\n",
    "\n",
    "# GeoPandas\n",
    "try:\n",
    "    import geopandas as gpd\n",
    "    print(\"GeoPandas found.\")\n",
    "except ImportError:\n",
    "    print(\"GeoPandas not found. Installing...\")\n",
    "    if install_package(\"geopandas\"):\n",
    "        import geopandas as gpd\n",
    "        print(\"GeoPandas successfully installed and imported.\")\n",
    "    else:\n",
    "        raise ImportError(\"Could not install GeoPandas. Please install manually.\")\n",
    "\n",
    "# SciPy\n",
    "try:\n",
    "    from scipy.ndimage import maximum_filter, minimum_filter, generic_filter\n",
    "    print(\"SciPy found.\")\n",
    "except ImportError:\n",
    "    print(\"SciPy not found. Installing...\")\n",
    "    if install_package(\"scipy\"):\n",
    "        from scipy.ndimage import maximum_filter, minimum_filter, generic_filter\n",
    "        print(\"SciPy successfully installed and imported.\")\n",
    "    else:\n",
    "        raise ImportError(\"Could not install SciPy. Please install manually.\")\n",
    "\n",
    "# Scikit-image\n",
    "try:\n",
    "    from skimage.transform import resize\n",
    "    print(\"Scikit-image found.\")\n",
    "except ImportError:\n",
    "    print(\"Scikit-image not found. Installing...\")\n",
    "    if install_package(\"scikit-image\"):\n",
    "        from skimage.transform import resize\n",
    "        print(\"Scikit-image successfully installed and imported.\")\n",
    "    else:\n",
    "        raise ImportError(\"Could not install Scikit-image. Please install manually.\")\n",
    "\n",
    "# --- GPU Libraries (Install Later via Environment) ---\n",
    "try:\n",
    "    import cupy as cp\n",
    "    import cupyx.scipy.ndimage as ndimage_gpu\n",
    "    from rmm.allocators.cupy import rmm_cupy_allocator\n",
    "    cp.cuda.set_allocator(rmm_cupy_allocator)\n",
    "    _HAS_CUDA = True\n",
    "    print(\"CUDA libraries found. GPU acceleration is available.\")\n",
    "    # Get properties of the first GPU\n",
    "    gpu = cp.cuda.Device(0)\n",
    "    print(f\"   GPU Name: {gpu.name}\")\n",
    "    print(f\"   GPU Memory: {gpu.mem_info[1] / 1024**3:.2f} GB total, {gpu.mem_info[0] / 1024**3:.2f} GB free\")\n",
    "except ImportError as e:\n",
    "    print(f\"CUDA libraries not found: {e}\")\n",
    "    print(\"Add 'cupy' and 'rmm' to environment.yml for GPU acceleration.\")\n",
    "    _HAS_CUDA = False\n",
    "\n",
    "# --- WhiteboxTools (Install Later via Environment) ---\n",
    "try:\n",
    "    from whitebox.whitebox_tools import WhiteboxTools\n",
    "    _HAS_WBT = True\n",
    "    print(\"WhiteboxTools found.\")\n",
    "except ImportError:\n",
    "    print(\"WhiteboxTools not found. Add 'whitebox' to environment.yml if needed.\")\n",
    "    _HAS_WBT = False\n",
    "\n",
    "# --- PyWavelets ---\n",
    "try:\n",
    "    import pywt\n",
    "    _HAS_PYWT = True\n",
    "    print(\"PyWavelets found.\")\n",
    "except ImportError:\n",
    "    print(\"PyWavelets not found. Installing...\")\n",
    "    if install_package(\"pywavelets\"):\n",
    "        try:\n",
    "            import pywt\n",
    "            _HAS_PYWT = True\n",
    "            print(\"PyWavelets successfully installed and imported.\")\n",
    "        except ImportError:\n",
    "            print(\"PyWavelets installed. Please restart kernel to use it.\")\n",
    "            _HAS_PYWT = False\n",
    "    else:\n",
    "        print(\"Could not install PyWavelets.\")\n",
    "        _HAS_PYWT = False\n",
    "\n",
    "# --- Scikit-TDA (Ripser + Persim) ---\n",
    "try:\n",
    "    import ripser\n",
    "    import persim\n",
    "    _HAS_SCIKIT_TDA = True\n",
    "    print(\"Scikit-TDA (ripser, persim) found.\")\n",
    "except ImportError:\n",
    "    print(\"Scikit-TDA libraries not found. Installing...\")\n",
    "    ripser_success = install_package(\"ripser\")\n",
    "    persim_success = install_package(\"persim\")\n",
    "\n",
    "    if ripser_success and persim_success:\n",
    "        try:\n",
    "            import ripser\n",
    "            import persim\n",
    "            _HAS_SCIKIT_TDA = True\n",
    "            print(\"Scikit-TDA successfully installed and imported.\")\n",
    "        except ImportError:\n",
    "            print(\"Scikit-TDA installed. Please restart kernel to use it.\")\n",
    "            _HAS_SCIKIT_TDA = False\n",
    "    else:\n",
    "        print(\"Could not install all Scikit-TDA components.\")\n",
    "        _HAS_SCIKIT_TDA = False\n",
    "\n",
    "# --- Robust Path Resolution ---\n",
    "def find_project_root(marker='README.md'):\n",
    "    \"\"\"Find the project root by searching upwards for a marker file.\"\"\"\n",
    "    path = Path.cwd().resolve()\n",
    "    while path.parent != path:\n",
    "        if (path / marker).exists():\n",
    "            return path\n",
    "        path = path.parent\n",
    "    raise FileNotFoundError(f\"Project root with marker '{marker}' not found.\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "PROC_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "# --- Analysis Grid & CRS ---\n",
    "TARGET_CRS = \"EPSG:5070\"  # NAD83 / Conus Albers\n",
    "TARGET_RES_M = 30.0\n",
    "\n",
    "# --- Covariate Parameters (Centralized Configuration) ---\n",
    "RELIEF_WINDOWS_M: List[int] = [90, 450, 1500]\n",
    "WAVELET: str = \"db2\"\n",
    "WAVELET_LEVELS: int = 3\n",
    "GAUSSIAN_SIGMA_M: float = 60.0\n",
    "MIN_SLOPE_DEG: float = 0.1\n",
    "TDA_N_SAMPLES: int = 5000  # Number of pixels to sample for TDA calculation\n",
    "\n",
    "# --- Inputs (from previous notebooks) ---\n",
    "DEM_TILES_DIR = PROC_DIR / \"dem_1arcsec_tiles\"\n",
    "AOI_GPKG_PATH = PROC_DIR / \"study_area_provinces.gpkg\"\n",
    "\n",
    "AOI_LAYER = \"provinces\"\n",
    "DEM_TILES = sorted(DEM_TILES_DIR.glob(\"*.tif\"))\n",
    "\n",
    "# --- Outputs ---\n",
    "COVARIATE_OUT_DIR = PROC_DIR / \"covariates\"\n",
    "COVARIATE_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ALIGNED_DEM_PATH = COVARIATE_OUT_DIR / \"dem_aligned.tif\"\n",
    "VRT_PATH = COVARIATE_OUT_DIR / \"source_dem.vrt\"\n",
    "SLOPE_PATH = COVARIATE_OUT_DIR / \"slope_deg.tif\"\n",
    "RELIEF_PATHS = {w: COVARIATE_OUT_DIR / f\"relief_{w}m.tif\" for w in RELIEF_WINDOWS_M}\n",
    "FILLED_PATH = COVARIATE_OUT_DIR / \"dem_filled.tif\"\n",
    "SMOOTH_PATH = COVARIATE_OUT_DIR / \"dem_smooth_for_hydro.tif\"\n",
    "TWI_PATH = COVARIATE_OUT_DIR / \"twi.tif\"\n",
    "SPI_PATH = COVARIATE_OUT_DIR / \"spi.tif\"\n",
    "ROUGH_PATHS = {\n",
    "    lvl: COVARIATE_OUT_DIR / f\"rough_{WAVELET}_L{lvl}.tif\"\n",
    "    for lvl in range(1, WAVELET_LEVELS + 1)\n",
    "}\n",
    "TDA_PIT_PATH = COVARIATE_OUT_DIR / \"tda_pit_persistence.tif\"\n",
    "TDA_RIDGE_PATH = COVARIATE_OUT_DIR / \"tda_ridge_persistence.tif\"\n",
    "\n",
    "# --- Centralized Tiling Configuration for GPU ---\n",
    "TILE_SIZE = 4096\n",
    "HALO = 256\n",
    "\n",
    "# --- Dask & Parallel Processing Configuration ---\n",
    "PHYSICAL_CORES = psutil.cpu_count(logical=False)\n",
    "AVAILABLE_MEMORY_BYTES = psutil.virtual_memory().available\n",
    "MEMORY_FOR_DASK_BYTES = AVAILABLE_MEMORY_BYTES * 0.80\n",
    "MEMORY_LIMIT_PER_WORKER = MEMORY_FOR_DASK_BYTES / PHYSICAL_CORES\n",
    "N_WORKERS = PHYSICAL_CORES\n",
    "SAFETY_FACTOR = 12\n",
    "BYTES_PER_PIXEL = np.dtype('float32').itemsize\n",
    "target_chunk_bytes = MEMORY_LIMIT_PER_WORKER / SAFETY_FACTOR\n",
    "pixels_per_chunk = target_chunk_bytes / BYTES_PER_PIXEL\n",
    "\n",
    "cluster = LocalCluster(\n",
    "    n_workers=N_WORKERS,\n",
    "    threads_per_worker=1,\n",
    "    memory_limit=MEMORY_LIMIT_PER_WORKER\n",
    ")\n",
    "client = Client(cluster)\n",
    "\n",
    "# --- Print Setup Summary ---\n",
    "print(\"\\n--- Configuration Summary ---\")\n",
    "print(f\"Project Root:        {PROJECT_ROOT}\")\n",
    "print(f\"Found {len(DEM_TILES)} DEM tiles.\")\n",
    "print(f\"Output Covariate Dir:  {COVARIATE_OUT_DIR}\")\n",
    "print(\"-\" * 29)\n",
    "print(f\"Dask Dashboard:      {client.dashboard_link}\")\n",
    "print(f\"Dask Cluster Size:   {N_WORKERS} workers @ {psutil._common.bytes2human(MEMORY_LIMIT_PER_WORKER)} each\")\n",
    "print(f\"Optional deps -> WhiteboxTools: {_HAS_WBT} | PyWavelets: {_HAS_PYWT} | Scikit-TDA: {_HAS_SCIKIT_TDA}\")\n",
    "print(\"-\" * 29)\n",
    "print(f\"GPU Available:       {_HAS_CUDA}\")\n",
    "print(f\"GPU Tile Size:       {TILE_SIZE}x{TILE_SIZE} with {HALO}px halo\")\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "if not all([_HAS_WBT, _HAS_PYWT, _HAS_SCIKIT_TDA, _HAS_CUDA]):\n",
    "    print(\"\\nSome packages missing. Consider adding to environment.yml and rebuilding.\")\n",
    "\n",
    "print(\"\\nSetup complete! Ready for geospatial analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7119eb554c561e98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T04:36:03.672090Z",
     "start_time": "2025-09-24T04:36:03.643097Z"
    }
   },
   "outputs": [],
   "source": "# === 1. Dynamic Resource Allocation & Dask Setup ===\n\n# --- Core Libraries ---\nimport os\nimport sys\nimport json\nimport warnings\nimport datetime as dt\nfrom pathlib import Path\nfrom typing import List, Dict, Optional\nimport time\n\nimport dask\nimport dask.array as da\nfrom dask.diagnostics import ProgressBar\nfrom dask.distributed import Client, LocalCluster\nfrom dask import delayed\n\nimport numpy as np\nimport rasterio\nfrom rasterio.windows import Window\nimport rioxarray as rxr\nimport xarray as xr\nimport geopandas as gpd\nimport psutil\nimport math\nimport subprocess\nfrom scipy.ndimage import maximum_filter, minimum_filter, generic_filter\nfrom skimage.transform import resize\n\ntry:\n    import cupy as cp\n    import cupyx.scipy.ndimage as ndimage_gpu\n    from rmm.allocators.cupy import rmm_cupy_allocator\n    cp.cuda.set_allocator(rmm_cupy_allocator)\n    _HAS_CUDA = True\n    print(\"✅ CUDA libraries found. GPU acceleration is available.\")\n    # Get properties of the first GPU\n    gpu = cp.cuda.Device(0)\n    print(f\"   GPU Name: {gpu.name}\")\n    print(f\"   GPU Memory: {gpu.mem_info[1] / 1024**3:.2f} GB total, {gpu.mem_info[0] / 1024**3:.2f} GB free\")\nexcept ImportError:\n    _HAS_CUDA = False\n    print(\"❌ CUDA libraries (cupy, rmm) not found. GPU acceleration is disabled.\")\n\n# --- Optional Heavy Dependencies (Checked at Runtime) ---\ntry:\n    from whitebox.whitebox_tools import WhiteboxTools\n    _HAS_WBT = True\nexcept ImportError:\n    _HAS_WBT = False\ntry:\n    import pywt\n    _HAS_PYWT = True\nexcept ImportError:\n    _HAS_PYWT = False\n# *** FIX: ADD THE SCIKIT-TDA CHECK HERE ***\ntry:\n    import ripser\n    import persim\n    _HAS_SCIKIT_TDA = True\nexcept ImportError:\n    _HAS_SCIKIT_TDA = False\n\n# --- Robust Path Resolution ---\ndef find_project_root(marker='README.md'):\n    \"\"\"Find the project root by searching upwards for a marker file.\"\"\"\n    path = Path.cwd().resolve()\n    while path.parent != path:\n        if (path / marker).exists():\n            return path\n        path = path.parent\n    raise FileNotFoundError(f\"Project root with marker '{marker}' not found.\")\n\nPROJECT_ROOT = find_project_root()\nDATA_DIR = PROJECT_ROOT / \"data\"\nPROC_DIR = DATA_DIR / \"processed\"\n\n# --- Analysis Grid & CRS ---\nTARGET_CRS = \"EPSG:5070\"  # NAD83 / Conus Albers\nTARGET_RES_M = 30.0\n\n# --- Covariate Parameters (Centralized Configuration) ---\nRELIEF_WINDOWS_M: List[int] = [90, 450, 1500]\nWAVELET: str = \"db2\"\nWAVELET_LEVELS: int = 3\nGAUSSIAN_SIGMA_M: float = 60.0\nMIN_SLOPE_DEG: float = 0.1\n# *** NEW: ADD TDA PARAMETER HERE ***\nTDA_N_SAMPLES: int = 5000 # Number of pixels to sample for TDA calculation\n\n# --- Inputs (from previous notebooks) ---\nDEM_TILES_DIR = PROC_DIR / \"dem_1arcsec_tiles\"\nAOI_GPKG_PATH = PROC_DIR / \"study_area_provinces.gpkg\"\n\nAOI_LAYER = \"provinces\"\nDEM_TILES = sorted(DEM_TILES_DIR.glob(\"*.tif\"))\n\n# --- Outputs (All defined here to avoid NameError later) ---\nCOVARIATE_OUT_DIR = PROC_DIR / \"covariates\"\nCOVARIATE_OUT_DIR.mkdir(parents=True, exist_ok=True)\nALIGNED_DEM_PATH = COVARIATE_OUT_DIR / \"dem_aligned.tif\"\nVRT_PATH = COVARIATE_OUT_DIR / \"source_dem.vrt\"\nSLOPE_PATH = COVARIATE_OUT_DIR / \"slope_deg.tif\"\nRELIEF_PATHS = {w: COVARIATE_OUT_DIR / f\"relief_{w}m.tif\" for w in RELIEF_WINDOWS_M}\nFILLED_PATH = COVARIATE_OUT_DIR / \"dem_filled.tif\"\nSMOOTH_PATH = COVARIATE_OUT_DIR / \"dem_smooth_for_hydro.tif\"\nTWI_PATH = COVARIATE_OUT_DIR / \"twi.tif\"\nSPI_PATH = COVARIATE_OUT_DIR / \"spi.tif\"\nROUGH_PATHS = {\n    lvl: COVARIATE_OUT_DIR / f\"rough_{WAVELET}_L{lvl}.tif\"\n    for lvl in range(1, WAVELET_LEVELS + 1)\n}\nTDA_PIT_PATH = COVARIATE_OUT_DIR / \"tda_pit_persistence.tif\"\nTDA_RIDGE_PATH = COVARIATE_OUT_DIR / \"tda_ridge_persistence.tif\"\n\n# --- Centralized Tiling Configuration for GPU ---\n# Define a tile size that will comfortably fit in GPU memory with overhead\n# 4096x4096 is a good starting point for GPUs with >12GB VRAM. Adjust if needed.\nTILE_SIZE = 4096\n# Define overlap/halo for focal operations to avoid edge artifacts\nHALO = 256\n\n# --- Dask & Parallel Processing Configuration ---\n# MEMORY OPTIMIZATION: Conservative settings for 64GB usable memory\nPHYSICAL_CORES = psutil.cpu_count(logical=False)\nTOTAL_USABLE_MEMORY_GB = 64  # Set to your safe memory limit\nTOTAL_USABLE_MEMORY_BYTES = TOTAL_USABLE_MEMORY_GB * 1024**3\n\n# Use only 50% of cores to reduce memory pressure\nN_WORKERS = max(PHYSICAL_CORES // 2, 2)\n\n# Reserve memory for OS and other processes\nMEMORY_FOR_DASK_BYTES = TOTAL_USABLE_MEMORY_BYTES * 0.70  # 70% of 64GB = ~45GB\nMEMORY_LIMIT_PER_WORKER = MEMORY_FOR_DASK_BYTES / N_WORKERS\n\n# More conservative safety factor for large rasters\nSAFETY_FACTOR = 20  # Increased from 12 to reduce chunk sizes\nBYTES_PER_PIXEL = np.dtype('float32').itemsize\ntarget_chunk_bytes = MEMORY_LIMIT_PER_WORKER / SAFETY_FACTOR\npixels_per_chunk = target_chunk_bytes / BYTES_PER_PIXEL\n\ncluster = LocalCluster(\n    n_workers=N_WORKERS,\n    threads_per_worker=1,\n    memory_limit=MEMORY_LIMIT_PER_WORKER\n)\nclient = Client(cluster)\n\n# --- Print Setup Summary ---\nprint(\"--- Configuration Summary ---\")\nprint(f\"Project Root:        {PROJECT_ROOT}\")\nprint(f\"Found {len(DEM_TILES)} DEM tiles.\")\nprint(f\"Output Covariate Dir:  {COVARIATE_OUT_DIR}\")\nprint(\"-\" * 29)\nprint(f\"Dask Dashboard:      {client.dashboard_link}\")\nprint(f\"Dask Cluster Size:   {N_WORKERS} workers @ {psutil._common.bytes2human(MEMORY_LIMIT_PER_WORKER)} each\")\nprint(f\"Total Memory Budget: {TOTAL_USABLE_MEMORY_GB}GB (usable), {MEMORY_FOR_DASK_BYTES / 1024**3:.1f}GB (for Dask)\")\nprint(f\"Optional deps -> WhiteboxTools: {_HAS_WBT} | PyWavelets: {_HAS_PYWT} | Scikit-TDA: {_HAS_SCIKIT_TDA}\")\nprint(\"-\" * 29)\nprint(f\"GPU Available:       {_HAS_CUDA}\")\nprint(f\"GPU Tile Size:       {TILE_SIZE}x{TILE_SIZE} with {HALO}px halo\")\nprint(\"-----------------------------\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb11aa16cd4ca42c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T17:19:50.868830Z",
     "start_time": "2025-09-23T17:19:50.865539Z"
    }
   },
   "outputs": [],
   "source": [
    "# CV-00b (optional): quick scanners to help locate DEMs\n",
    "def scan_for_geotiffs(root=\".\", max_print=25):\n",
    "    root = Path(root)\n",
    "    if not root.exists():\n",
    "        print(f\"scan_for_geotiffs: '{root}' does not exist.\")\n",
    "        return []\n",
    "    found = [p for p in root.rglob(\"*\") if p.suffix.lower() in {\".tif\",\".tiff\",\".vrt\"}]\n",
    "    print(f\"Found {len(found)} candidate DEM file(s) under '{root}'.\")\n",
    "    for p in found[:max_print]:\n",
    "        print(\" -\", p)\n",
    "    if len(found) > max_print:\n",
    "        print(\" ... (truncated)\")\n",
    "    return found\n",
    "\n",
    "# Examples:\n",
    "# scan_for_geotiffs(\"data\")\n",
    "# scan_for_geotiffs(\"D:/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b9e37e661dc4da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T17:34:03.416668Z",
     "start_time": "2025-09-23T17:19:50.883259Z"
    }
   },
   "outputs": [],
   "source": "# === 2. Build Aligned Analysis-Ready DEM ===\n# This cell creates a single, analysis-ready DEM by:\n#  1. Building a VRT from the downloaded tiles for efficient access.\n#  2. Warping (reprojecting, clipping, resampling) the VRT to the final analysis grid.\n\nif not ALIGNED_DEM_PATH.exists():\n    if not DEM_TILES:\n        raise FileNotFoundError(f\"No DEM tiles found in {DEM_TILES_DIR}. Please run the previous notebook.\")\n\n    # 1. Build VRT from all source DEM tiles\n    print(f\"Building VRT from {len(DEM_TILES)} tiles -> {VRT_PATH}\")\n    input_list_path = COVARIATE_OUT_DIR / \"gdal_input_file_list.txt\"\n    with open(input_list_path, 'w') as f:\n        for tile_path in DEM_TILES:\n            f.write(f\"{str(tile_path)}\\n\")\n\n    gdalbuildvrt_command = [\n        'gdalbuildvrt', '-input_file_list', str(input_list_path), str(VRT_PATH)\n    ]\n\n    try:\n        subprocess.run(gdalbuildvrt_command, check=True, capture_output=True, text=True)\n        print(\"...VRT built successfully.\")\n    except subprocess.CalledProcessError as e:\n        print(\"--- GDAL VRT ERROR ---\"); print(e.stderr); raise\n    finally:\n        if input_list_path.exists(): input_list_path.unlink()\n\n    # --- FIX FOR REPROJECTION MEMORY ERROR ---\n    # 2. Use gdalwarp for memory-efficient reprojection, clipping, and resampling.\n    print(f\"Warping VRT to target grid using gdalwarp: {TARGET_CRS} @ {TARGET_RES_M}m resolution.\")\n\n    # Load the Area of Interest to get the target bounds\n    aoi_gdf = gpd.read_file(AOI_GPKG_PATH, layer=AOI_LAYER).to_crs(TARGET_CRS)\n    bounds = aoi_gdf.total_bounds\n\n    # Build the gdalwarp command with memory-efficient settings\n    gdalwarp_command = [\n        'gdalwarp',\n        '-t_srs', TARGET_CRS,       # Target CRS\n        '-tr', str(TARGET_RES_M), str(TARGET_RES_M), # Target resolution\n        '-te', str(bounds[0]), str(bounds[1]), str(bounds[2]), str(bounds[3]), # Target extent\n        '-r', 'bilinear',           # Resampling method\n        '-co', 'COMPRESS=LZW',      # Output compression\n        '-co', 'TILED=YES',         # Output tiling\n        '-co', 'BLOCKXSIZE=512',    # Smaller block size for memory efficiency\n        '-co', 'BLOCKYSIZE=512',    # Smaller block size for memory efficiency\n        '-co', 'BIGTIFF=IF_SAFER',\n        '-wm', '2048',              # Limit warp memory to 2GB\n        '-multi',                   # Use multithreading\n        '-wo', 'NUM_THREADS=ALL_CPUS',\n        '-of', 'GTiff',             # Output format\n        '-overwrite',               # Overwrite if exists\n        str(VRT_PATH),              # Input VRT\n        str(ALIGNED_DEM_PATH)       # Output file\n    ]\n\n    try:\n        # This is a long-running process, so we don't capture output unless there's an error.\n        # This allows GDAL's progress bar to print to the console.\n        print(\"Writing final aligned DEM (this may take several minutes)...\")\n        subprocess.run(gdalwarp_command, check=True)\n        print(\"...Warping complete.\")\n    except subprocess.CalledProcessError as e:\n        print(\"--- GDAL WARP ERROR ---\"); print(e.stderr); raise\n    # --- END FIX ---\nelse:\n    print(f\"Using existing analysis-ready DEM: {ALIGNED_DEM_PATH}\")\n\n# --- OPTIMIZED CHUNK CALCULATION ---\nprint(\"Calculating optimal chunk size for Dask (memory-efficient)...\")\nwith rasterio.open(ALIGNED_DEM_PATH) as src:\n    # Get native block size from the file\n    block_y, block_x = src.block_shapes[0]\n    print(f\"Native block size: {block_y} x {block_x}\")\n    \n    # Calculate chunk size based on conservative memory limits\n    # Target: each chunk should be ~200MB to allow for intermediate operations\n    TARGET_CHUNK_MB = 200\n    target_chunk_bytes = TARGET_CHUNK_MB * 1024 * 1024\n    pixels_per_chunk = target_chunk_bytes / BYTES_PER_PIXEL\n    chunk_dim_raw = int(math.sqrt(pixels_per_chunk))\n    \n    # Align to block size for efficient I/O\n    ALIGNMENT = 512  # Standard GeoTIFF block size\n    chunk_dim_aligned = int(round(chunk_dim_raw / ALIGNMENT) * ALIGNMENT)\n    chunk_dim_aligned = max(chunk_dim_aligned, ALIGNMENT)\n    chunk_dim_aligned = min(chunk_dim_aligned, 4096)  # Cap at 4096 for safety\n    \n    OPTIMAL_CHUNKS = {'y': chunk_dim_aligned, 'x': chunk_dim_aligned}\n    chunk_memory_mb = (chunk_dim_aligned ** 2 * BYTES_PER_PIXEL) / (1024 * 1024)\n    print(f\"Calculated Optimal Chunk Size: {OPTIMAL_CHUNKS}\")\n    print(f\"Per-chunk memory: {chunk_memory_mb:.1f} MB\")\n    print(f\"Max concurrent chunks with {N_WORKERS} workers: ~{int(MEMORY_FOR_DASK_BYTES / (chunk_memory_mb * 1024 * 1024))}\")\n# --- END OPTIMIZED CHUNK CALCULATION ---\n\n# Load the final, aligned DEM as a Dask-backed DataArray for all subsequent steps\n# Use smaller chunks to prevent memory overflow\ndem_aligned = rxr.open_rasterio(ALIGNED_DEM_PATH, chunks=OPTIMAL_CHUNKS).squeeze(\"band\", drop=True)\nprint(\"\\nFinal Analysis-Ready DEM:\")\nprint(dem_aligned)\nprint(f\"\\nEstimated memory if fully loaded: {dem_aligned.nbytes / (1024**3):.2f} GB\")\nprint(f\"Number of chunks: {len(dem_aligned.data.chunks[0]) * len(dem_aligned.data.chunks[1])}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df1f812c2b18aa0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T17:34:03.636107Z",
     "start_time": "2025-09-23T17:34:03.629386Z"
    }
   },
   "outputs": [],
   "source": [
    "# === REVISED CELL 1: FINALIZED TDA Helper Functions ===\n",
    "\n",
    "from ripser import ripser\n",
    "from persim.landscapes import PersLandscapeExact\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "def calculate_tda_pit_persistence(dem_chunk: np.ndarray, n_samples: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates a local TDA pit persistence metric.\n",
    "\n",
    "    **CRITICAL**: This function MUST return a 2D NumPy array of shape (1, 1)\n",
    "    to be compatible with the `chunks` argument in `dask.array.map_blocks`.\n",
    "    \"\"\"\n",
    "    # ... (all the sampling and ripser logic is the same as before) ...\n",
    "    # ... we just change what is returned ...\n",
    "\n",
    "    if np.all(np.isnan(dem_chunk)) or dem_chunk.shape[0] < 10 or dem_chunk.shape[1] < 10:\n",
    "        return np.array([[np.nan]], dtype=np.float32)\n",
    "\n",
    "    height_values = dem_chunk[~np.isnan(dem_chunk)].reshape(-1, 1)\n",
    "    if height_values.shape[0] < 10:\n",
    "        return np.array([[np.nan]], dtype=np.float32)\n",
    "\n",
    "    if height_values.shape[0] > n_samples:\n",
    "        step = int(np.ceil(height_values.shape[0] / n_samples))\n",
    "        points_sampled = height_values[::step, :]\n",
    "    else:\n",
    "        points_sampled = height_values\n",
    "\n",
    "    try:\n",
    "        diagrams = ripser(points_sampled, maxdim=0)['dgms']\n",
    "        if len(diagrams) == 0 or diagrams[0].shape[0] == 0:\n",
    "            return np.array([[0.0]], dtype=np.float32)\n",
    "\n",
    "        h0_diagram = diagrams[0]\n",
    "        finite_h0 = h0_diagram[np.isfinite(h0_diagram[:, 1])]\n",
    "\n",
    "        if finite_h0.shape[0] == 0:\n",
    "            return np.array([[0.0]], dtype=np.float32)\n",
    "\n",
    "        ple = PersLandscapeExact(dgms=[finite_h0], hom_deg=0)\n",
    "        persistence_summary = ple.p_norm(p=1)\n",
    "\n",
    "    except (ValueError, RuntimeError) as e:\n",
    "        warnings.warn(f\"TDA calculation failed on a chunk and will be replaced with NaN. Error: {e}\")\n",
    "        persistence_summary = np.nan\n",
    "\n",
    "    # **THE FIX**: Return a (1, 1) NumPy array to match the output chunks.\n",
    "    return np.array([[persistence_summary]], dtype=np.float32)\n",
    "\n",
    "\n",
    "def calculate_tda_ridge_persistence(dem_chunk: np.ndarray, n_samples: int) -> np.ndarray:\n",
    "    \"\"\"Calculates TDA ridge persistence by inverting the DEM.\"\"\"\n",
    "    inverted_chunk = dem_chunk * -1.0\n",
    "    return calculate_tda_pit_persistence(inverted_chunk, n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0b351bd5ae14cc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T17:35:42.174475Z",
     "start_time": "2025-09-23T17:34:03.640658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STAGE 1: Generating low-resolution TDA summary rasters ---\n",
      "GRAPHING: Low-res TDA Pit Persistence -> tda_pit_persistence_low_res.tif\n",
      "GRAPHING: Low-res TDA Ridge Persistence -> tda_ridge_persistence_low_res.tif\n",
      "\n",
      "Executing 2 TDA intermediate tasks...\n",
      "...Stage 1 complete.\n"
     ]
    }
   ],
   "source": [
    "# === CELL 5: STAGE 1 - GENERATE INTERMEDIATE TDA RASTERS ===\n",
    "\n",
    "print(\"--- STAGE 1: Generating low-resolution TDA summary rasters ---\")\n",
    "tda_intermediate_tasks = []\n",
    "\n",
    "if _HAS_SCIKIT_TDA:\n",
    "    TDA_PIT_LOW_RES_PATH = COVARIATE_OUT_DIR / \"tda_pit_persistence_low_res.tif\"\n",
    "    TDA_RIDGE_LOW_RES_PATH = COVARIATE_OUT_DIR / \"tda_ridge_persistence_low_res.tif\"\n",
    "\n",
    "    # Define the explicit output chunking scheme\n",
    "    num_y_blocks = len(dem_aligned.chunksizes['y'])\n",
    "    num_x_blocks = len(dem_aligned.chunksizes['x'])\n",
    "    output_chunks = ((1,) * num_y_blocks, (1,) * num_x_blocks)\n",
    "\n",
    "    # Calculate coords once to reuse\n",
    "    chunk_sizes_y = dem_aligned.chunksizes['y']\n",
    "    chunk_sizes_x = dem_aligned.chunksizes['x']\n",
    "    y_mid_indices = np.cumsum(chunk_sizes_y) - np.array(chunk_sizes_y) / 2\n",
    "    x_mid_indices = np.cumsum(chunk_sizes_x) - np.array(chunk_sizes_x) / 2\n",
    "    y_coords = dem_aligned.y.isel(y=y_mid_indices.astype(int)).data\n",
    "    x_coords = dem_aligned.x.isel(x=x_mid_indices.astype(int)).data\n",
    "\n",
    "    # --- Task for TDA Pit Persistence ---\n",
    "    if not TDA_PIT_LOW_RES_PATH.exists():\n",
    "        print(f\"GRAPHING: Low-res TDA Pit Persistence -> {TDA_PIT_LOW_RES_PATH.name}\")\n",
    "        tda_low_res_dask = da.map_blocks(\n",
    "            calculate_tda_pit_persistence, dem_aligned.data,\n",
    "            n_samples=TDA_N_SAMPLES, dtype=np.float32, chunks=output_chunks\n",
    "        )\n",
    "        tda_low_res_da = xr.DataArray(\n",
    "            tda_low_res_dask, dims=('y', 'x'), coords={'y': y_coords, 'x': x_coords}\n",
    "        )\n",
    "        tda_low_res_da.rio.write_crs(dem_aligned.rio.crs, inplace=True)\n",
    "\n",
    "        # Get the lazy write task object by passing compute=False\n",
    "        pit_write_task = tda_low_res_da.rio.to_raster(\n",
    "            TDA_PIT_LOW_RES_PATH, tiled=True, lock=True, compress='LZW', windowed=True, compute=False\n",
    "        )\n",
    "        tda_intermediate_tasks.append(pit_write_task)\n",
    "    else:\n",
    "        print(f\"EXISTS: Low-res TDA Pit Persistence\")\n",
    "\n",
    "    # --- Task for TDA Ridge Persistence ---\n",
    "    if not TDA_RIDGE_LOW_RES_PATH.exists():\n",
    "        print(f\"GRAPHING: Low-res TDA Ridge Persistence -> {TDA_RIDGE_LOW_RES_PATH.name}\")\n",
    "        tda_low_res_ridge_dask = da.map_blocks(\n",
    "            calculate_tda_ridge_persistence, dem_aligned.data,\n",
    "            n_samples=TDA_N_SAMPLES, dtype=np.float32, chunks=output_chunks\n",
    "        )\n",
    "        tda_low_res_ridge_da = xr.DataArray(\n",
    "            tda_low_res_ridge_dask, dims=('y', 'x'), coords={'y': y_coords, 'x': x_coords}\n",
    "        )\n",
    "        tda_low_res_ridge_da.rio.write_crs(dem_aligned.rio.crs, inplace=True)\n",
    "\n",
    "        ridge_write_task = tda_low_res_ridge_da.rio.to_raster(\n",
    "            TDA_RIDGE_LOW_RES_PATH, tiled=True, lock=True, compress='LZW', windowed=True, compute=False\n",
    "        )\n",
    "        tda_intermediate_tasks.append(ridge_write_task)\n",
    "    else:\n",
    "        print(f\"EXISTS: Low-res TDA Ridge Persistence\")\n",
    "\n",
    "# --- Execute only the TDA intermediate tasks ---\n",
    "if tda_intermediate_tasks:\n",
    "    print(f\"\\nExecuting {len(tda_intermediate_tasks)} TDA intermediate tasks...\")\n",
    "    with ProgressBar():\n",
    "        dask.compute(*tda_intermediate_tasks)\n",
    "    print(\"...Stage 1 complete.\")\n",
    "else:\n",
    "    print(\"\\nAll TDA intermediate files already exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308625e95af35fe",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-09-23T17:35:42.398165Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 2: Generating all final, full-resolution covariates ---\n",
      "--- Defining Final Covariate Generation Tasks ---\n",
      "GRAPHING: Slope -> slope_deg.tif\n",
      "GRAPHING: Relief 90m -> relief_90m.tif\n",
      "GRAPHING: Relief 450m -> relief_450m.tif\n",
      "GRAPHING: Relief 1500m -> relief_1500m.tif\n",
      "GRAPHING: Wavelet Roughness (L1) -> rough_db2_L1.tif\n",
      "GRAPHING: Wavelet Roughness (L2) -> rough_db2_L2.tif\n",
      "GRAPHING: Wavelet Roughness (L3) -> rough_db2_L3.tif\n",
      "\n",
      "--- Executing Synchronous WhiteboxTools Processes ---\n",
      "WBT: Breaching depressions...\n"
     ]
    }
   ],
   "source": [
    "# === CELL 6: STAGE 2 - GENERATE ALL FINAL COVARIATES ===\n",
    "import numpy as np\n",
    "import pywt\n",
    "from scipy.ndimage import generic_filter\n",
    "from skimage.transform import resize\n",
    "\n",
    "print(\"\\n--- STAGE 2: Generating all final, full-resolution covariates ---\")\n",
    "final_write_tasks = []\n",
    "wbt_tasks = [] # Keep wbt tasks separate as they are handled differently\n",
    "\n",
    "# --- Helper function for wavelet roughness (already defined in your notebook) ---\n",
    "# NOTE: This is the optimized version from our previous discussion.\n",
    "def calculate_wavelet_roughness(dem_chunk: np.ndarray, wavelet_name: str, level: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates wavelet-based roughness. This version is optimized for memory and\n",
    "    uses bilinear resizing for a smoother output.\n",
    "    \"\"\"\n",
    "    if np.all(np.isnan(dem_chunk)):\n",
    "        return dem_chunk.astype(np.float32)\n",
    "\n",
    "    # Pad to reduce edge artifacts during wavelet transform\n",
    "    pad_width = 16\n",
    "    padded_chunk = np.pad(dem_chunk, pad_width=pad_width, mode='reflect')\n",
    "\n",
    "    # Decompose the signal into different frequency bands\n",
    "    coeffs = pywt.wavedec2(padded_chunk, wavelet=wavelet_name, level=level, mode='symmetric')\n",
    "    cH, cV, cD = coeffs[-level] # Get detail coefficients at the desired scale\n",
    "\n",
    "    # Calculate energy (magnitude) of the detail coefficients\n",
    "    energy_at_scale = np.sqrt(cH**2 + cV**2 + cD**2)\n",
    "\n",
    "    # Calculate local standard deviation of energy as the roughness metric\n",
    "    roughness_at_scale = generic_filter(energy_at_scale, np.std, size=3, mode='reflect')\n",
    "\n",
    "    # Use skimage.resize for efficient, smooth upsampling (bilinear interpolation)\n",
    "    roughness_resized = resize(\n",
    "        roughness_at_scale,\n",
    "        padded_chunk.shape,\n",
    "        order=1,\n",
    "        preserve_range=True,\n",
    "        anti_aliasing=True\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    # Un-pad the result to match the original chunk shape\n",
    "    return roughness_resized[pad_width:-pad_width, pad_width:-pad_width]\n",
    "\n",
    "print(\"--- Defining Final Covariate Generation Tasks ---\")\n",
    "\n",
    "# --- Tier 1: Foundational Covariates ---\n",
    "if not SLOPE_PATH.exists():\n",
    "    print(f\"GRAPHING: Slope -> {SLOPE_PATH.name}\")\n",
    "    slope_deg = xrs_slope(dem_aligned, name='slope_deg')\n",
    "    task = slope_deg.rio.to_raster(SLOPE_PATH, tiled=True, lock=True, compress='LZW', windowed=True, compute=False)\n",
    "    final_write_tasks.append(task)\n",
    "else: print(f\"EXISTS: Slope\")\n",
    "\n",
    "for w in RELIEF_WINDOWS_M:\n",
    "    out_path = RELIEF_PATHS[w]\n",
    "    if not out_path.exists():\n",
    "        print(f\"GRAPHING: Relief {w}m -> {out_path.name}\")\n",
    "        win_pix = max(3, round(w / TARGET_RES_M)); win_pix += (win_pix % 2 == 0)\n",
    "        depth = win_pix // 2\n",
    "        def relief_on_chunk(chunk, window_size):\n",
    "            return maximum_filter(chunk, size=window_size, mode='reflect') - minimum_filter(chunk, size=window_size, mode='reflect')\n",
    "        relief_dask_array = da.map_overlap(relief_on_chunk, dem_aligned.data, depth=depth, boundary='reflect', dtype=np.float32, window_size=win_pix)\n",
    "        relief_da = xr.DataArray(relief_dask_array, coords=dem_aligned.coords, name=f\"relief_{w}m\")\n",
    "        task = relief_da.rio.to_raster(out_path, tiled=True, lock=True, compress='LZW', windowed=True, compute=False)\n",
    "        final_write_tasks.append(task)\n",
    "    else: print(f\"EXISTS: Relief {w}m\")\n",
    "\n",
    "if _HAS_PYWT:\n",
    "    for lvl in range(1, WAVELET_LEVELS + 1):\n",
    "        out_path = ROUGH_PATHS[lvl]\n",
    "        if not out_path.exists():\n",
    "            print(f\"GRAPHING: Wavelet Roughness (L{lvl}) -> {out_path.name}\")\n",
    "            depth = 32 # Heuristic overlap for padding + filter\n",
    "            roughness_dask_array = da.map_overlap(calculate_wavelet_roughness, dem_aligned.data, depth=depth, boundary='reflect', dtype=np.float32, wavelet_name=WAVELET, level=lvl)\n",
    "            rough_xra = xr.DataArray(roughness_dask_array, coords=dem_aligned.coords, name=f\"rough_L{lvl}\")\n",
    "            task = rough_xra.rio.to_raster(out_path, tiled=True, lock=True, compress='LZW', windowed=True, compute=False)\n",
    "            final_write_tasks.append(task)\n",
    "        else: print(f\"EXISTS: Wavelet Roughness L{lvl}\")\n",
    "else: print(\"SKIPPING: Wavelet Roughness (pywt not found)\")\n",
    "\n",
    "# --- Tier 2: Process-Based Metrics (Executed Synchronously) ---\n",
    "print(\"\\n--- Executing Synchronous WhiteboxTools Processes ---\")\n",
    "if _HAS_WBT:\n",
    "    wbt = WhiteboxTools(); wbt.verbose = False\n",
    "    # The DEM needs to be filled and smoothed before TWI/SPI can be calculated.\n",
    "    # These steps are run sequentially as they depend on each other.\n",
    "    if not FILLED_PATH.exists():\n",
    "        print(\"WBT: Breaching depressions...\")\n",
    "        wbt.breach_depressions_least_cost(dem=str(ALIGNED_DEM_PATH), output=str(FILLED_PATH), dist=5, fill=True)\n",
    "    if not SMOOTH_PATH.exists():\n",
    "        print(\"WBT: Smoothing DEM for hydro analysis...\")\n",
    "        wbt.gaussian_filter(i=str(FILLED_PATH), output=str(SMOOTH_PATH), sigma=(GAUSSIAN_SIGMA_M / TARGET_RES_M))\n",
    "    if not TWI_PATH.exists():\n",
    "        print(\"WBT: Calculating TWI...\")\n",
    "        wbt.topographic_wetness_index(i=str(SMOOTH_PATH), output=str(TWI_PATH))\n",
    "    if not SPI_PATH.exists():\n",
    "        print(\"WBT: Calculating SPI...\")\n",
    "        wbt.stream_power_index(i=str(SMOOTH_PATH), output=str(SPI_PATH))\n",
    "    print(\"...WhiteboxTools processing complete.\")\n",
    "else:\n",
    "    print(\"SKIPPING: Tier 2 Metrics (whitebox-tools not found)\")\n",
    "\n",
    "# --- Tier 3: Upsampling TDA Summaries ---\n",
    "if _HAS_SCIKIT_TDA:\n",
    "    @dask.delayed\n",
    "    def run_gdal_upsample(in_path, out_path, template_path):\n",
    "        \"\"\"Upsamples a low-res raster to match the template's grid using gdalwarp.\"\"\"\n",
    "        if Path(out_path).exists(): return True # Skip if already done\n",
    "        print(f\"GDAL_WARP TASK: Upsampling {in_path.name} -> {out_path.name}\")\n",
    "\n",
    "        with rasterio.open(template_path) as t:\n",
    "            bounds = t.bounds\n",
    "            res_x, res_y = t.res\n",
    "\n",
    "        gdalwarp_cmd = [\n",
    "            'gdalwarp', '-r', 'bilinear', '-multi',\n",
    "            '-tr', str(res_x), str(abs(res_y)),\n",
    "            '-te', str(bounds.left), str(bounds.bottom), str(bounds.right), str(bounds.top),\n",
    "            '-co', 'COMPRESS=LZW', '-co', 'TILED=YES', '-co', 'BIGTIFF=IF_SAFER',\n",
    "            '-overwrite', str(in_path), str(out_path)\n",
    "        ]\n",
    "        try:\n",
    "            subprocess.run(gdalwarp_cmd, check=True, capture_output=True, text=True)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"--- GDAL WARP (Upsample) ERROR for {out_path.name} ---\\n{e.stderr}\")\n",
    "            raise\n",
    "        return True\n",
    "\n",
    "    # Define and add the upsampling tasks to the Dask graph\n",
    "    print(\"GRAPHING: TDA Upsampling tasks using gdalwarp...\")\n",
    "    task_pit_upsample = run_gdal_upsample(TDA_PIT_LOW_RES_PATH, TDA_PIT_PATH, ALIGNED_DEM_PATH)\n",
    "    task_ridge_upsample = run_gdal_upsample(TDA_RIDGE_LOW_RES_PATH, TDA_RIDGE_PATH, ALIGNED_DEM_PATH)\n",
    "    final_write_tasks.extend([task_pit_upsample, task_ridge_upsample])\n",
    "else:\n",
    "    print(\"SKIPPING: TDA Upsampling (scikit-tda not found)\")\n",
    "\n",
    "# --- Execute All Final Tasks ---\n",
    "all_final_tasks = final_write_tasks\n",
    "if all_final_tasks:\n",
    "    print(f\"\\nExecuting a total of {len(all_final_tasks)} final tasks...\")\n",
    "    with ProgressBar():\n",
    "        dask.compute(*all_final_tasks)\n",
    "    print(\"...Stage 2 complete. All covariates generated.\")\n",
    "else:\n",
    "    print(\"\\nAll final covariate layers already exist. Skipping generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a67d7e3bfde4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV-08: quick sanity plots (optional); safe to skip in headless runs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def quickplot(path: Path, title: str):\n",
    "    da = rxr.open_rasterio(path).squeeze(\"band\", drop=True)\n",
    "    fig, ax = plt.subplots(figsize=(6,5))\n",
    "    im = ax.imshow(da.data, origin=\"upper\")\n",
    "    ax.set_title(title)\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "# Examples:\n",
    "# quickplot(SLOPE_PATH, \"Slope (deg)\")\n",
    "# quickplot(list(RELIEF_PATHS.values())[0], \"Local relief (fine)\")\n",
    "# if _HAS_PYWT: quickplot(ROUGH_PATHS[0], \"Wavelet roughness (L1)\")\n",
    "# if _HAS_WBT: quickplot(TWI_PATH, \"Topographic Wetness Index (TWI)\")\n",
    "# if _HAS_WBT: quickplot(SPI_PATH, \"Stream Power Index (SPI)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5d05a0fa3c6860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. Final Manifest ===\n",
    "# This cell creates a final manifest file to document the run.\n",
    "\n",
    "# Collect all output file paths for the manifest\n",
    "output_files = {\n",
    "    \"dem_aligned\": ALIGNED_DEM_PATH,\n",
    "    \"dem_vrt\": VRT_PATH,\n",
    "    \"slope_deg\": SLOPE_PATH,\n",
    "}\n",
    "output_files.update({f\"relief_{w}m\": p for w, p in RELIEF_PATHS.items()})\n",
    "\n",
    "# Placeholder for future outputs\n",
    "if _HAS_PYWT:\n",
    "    output_files.update({f\"roughness_{WAVELET}_L{l}\": ROUGH_PATHS[l] for l in range(1, WAVELET_LEVELS + 1)})\n",
    "if _HAS_WBT:\n",
    "    output_files.update({\n",
    "        \"dem_filled\": FILLED_PATH,\n",
    "        \"dem_smooth_for_hydro\": SMOOTH_PATH,\n",
    "        \"twi\": TWI_PATH,\n",
    "        \"spi\": SPI_PATH,\n",
    "    })\n",
    "\n",
    "manifest = {\n",
    "    \"created_utc\": dt.datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"notebook\": \"1.4_covariate_process_layers.ipynb\",\n",
    "    \"config\": {\n",
    "        \"target_crs\": TARGET_CRS,\n",
    "        \"target_resolution_m\": TARGET_RES_M,\n",
    "        \"relief_windows_m\": RELIEF_WINDOWS_M,\n",
    "        \"wavelet\": WAVELET,\n",
    "        \"wavelet_levels\": WAVELET_LEVELS,\n",
    "        \"gaussian_sigma_m\": GAUSSIAN_SIGMA_M,\n",
    "        \"min_slope_deg\": MIN_SLOPE_DEG,\n",
    "        \"dask_cluster\": {\n",
    "            \"n_workers\": N_WORKERS,\n",
    "            \"memory_limit_per_worker\": str(psutil._common.bytes2human(MEMORY_LIMIT_PER_WORKER)),\n",
    "        },\n",
    "    },\n",
    "    \"inputs\": {\n",
    "        \"dem_tiles_dir\": str(DEM_TILES_DIR),\n",
    "        \"num_dem_tiles\": len(DEM_TILES),\n",
    "        \"aoi_gpkg\": str(AOI_GPKG_PATH),\n",
    "        \"aoi_layer\": AOI_LAYER,\n",
    "    },\n",
    "    \"outputs\": {k: str(v) for k, v in output_files.items()},\n",
    "    \"software\": {\n",
    "        \"python_version\": sys.version,\n",
    "        \"rasterio\": rasterio.__version__,\n",
    "        \"xarray\": xr.__version__,\n",
    "        \"rioxarray\": rxr.__version__,\n",
    "        \"geopandas\": gpd.__version__,\n",
    "        \"numpy\": np.__version__,\n",
    "        \"pywavelets\": (pywt.__version__ if _HAS_PYWT else \"not found\"),\n",
    "        \"whitebox\": (\"installed\" if _HAS_WBT else \"not found\")\n",
    "    }\n",
    "}\n",
    "\n",
    "manifest_path = COVARIATE_OUT_DIR / \"manifest_1_4_covariates.json\"\n",
    "with open(manifest_path, \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(f\"Wrote manifest: {manifest_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7759d2e61b4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. (Optional) Final Output Validation ===\n",
    "# This cell verifies that all expected files were created.\n",
    "\n",
    "print(\"--- Verifying Output Files ---\")\n",
    "all_output_paths = {**RELIEF_PATHS, **ROUGH_PATHS, \"slope\": SLOPE_PATH}\n",
    "if _HAS_WBT:\n",
    "    all_output_paths.update({\"twi\": TWI_PATH, \"spi\": SPI_PATH})\n",
    "if _HAS_SCIKIT_TDA:\n",
    "     all_output_paths.update({\"tda_pit_persistence\": TDA_PIT_PATH})\n",
    "\n",
    "success_count = 0\n",
    "fail_count = 0\n",
    "\n",
    "for name, path in all_output_paths.items():\n",
    "    if path.exists() and path.stat().st_size > 0:\n",
    "        print(f\"  [ OK ] {name:<25} -> {path.name}\")\n",
    "        success_count += 1\n",
    "    else:\n",
    "        print(f\"  [FAIL] {name:<25} -> FILE NOT FOUND OR EMPTY\")\n",
    "        fail_count += 1\n",
    "\n",
    "print(\"-\" * 30)\n",
    "if fail_count == 0:\n",
    "    print(f\"✅ Success! All {success_count} expected files were generated.\")\n",
    "else:\n",
    "    print(f\"❌ Warning: {fail_count} file(s) are missing or empty. Please review logs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}