# Protocol: Acquiring Raw Data for WEPP Modeling

**Document Version:** 1.0
**Date:** September 27, 2025
**Associated Project Task:** 1.3 - Acquire Raw Data: Download all necessary data for WEPP modeling (DEMs, climate, soils).

---

## 1. Objective

To acquire all necessary raw geospatial datasets required to parameterize and execute the Water Erosion Prediction Project (WEPP) model. This protocol outlines the authoritative sources, selection criteria, and download procedures for the three core data categories: topography (DEMs), climate, and soils.

## 2. Rationale and Strategic Justification

The scientific rigor of this project hinges on the quality of its target variable: the process-based erosion estimates from WEPP. Using a state-of-the-art, physically-based model like WEPP, rather than a simpler empirical model, provides a more fundamental and defensible test of the core process-prediction hypothesis ($H_2$) [cite: Project Proposal Fall 2025.md]. This task is therefore not merely logistical; it is a foundational step in ensuring the integrity of the entire predictive modeling workflow. Acquiring data from consistent, national-scale, authoritative sources guarantees that the inputs are of the highest possible quality and that the entire process is transparent and reproducible.

## 3. Authoritative Data Sources and Selection

### 3.1. Topography: Digital Elevation Models (DEMs)

* **Primary Dataset:** **USGS 3D Elevation Program (3DEP) Seamless DEMs**.
* **Resolution:** **1/3 arc-second (~10 meters)**. This resolution provides an optimal balance between capturing significant topographic detail and maintaining a manageable data volume for regional-scale analysis [cite: Geomorphic Analysis Framework Development.md].
* **Acquisition Portal:** **The National Map (TNM) Downloader**.
    * *Direct Link:* [https://apps.nationalmap.gov/downloader/](https://apps.nationalmap.gov/downloader/)
* **Selection Rationale:** The 3DEP dataset is the gold standard for bare-earth elevation in the United States, derived primarily from high-resolution LiDAR. Its use is essential for accurately calculating the topographic inputs for WEPP, such as slope gradient and length.

### 3.2. Climate Data for WEPP (`.cli` files)

WEPP requires daily climate statistics (e.g., precipitation, temperature) formatted into specific `.cli` files. While custom files can be generated, a more efficient starting point is to use pre-generated, gridded climate generator data.

* **Primary Dataset:** **Rock:Clime - WEPP Gridded Climate Data**. This tool provides access to CLIGEN-calibrated weather station data interpolated onto a grid for the entire US.
* **Acquisition Portal:** **USDA Agricultural Research Service (ARS) WEPP Interface**.
    * *Direct Link:* [https://wepp.ars.usda.gov/weppdocs/rockclime/rockclime.html](https://wepp.ars.usda.gov/weppdocs/rockclime/rockclime.html)
* **Selection Rationale:** Using the official USDA-provided climate data ensures that the climate inputs are consistent with the WEPP model's internal parameterizations and calibration. This is the most direct and reliable method for acquiring WEPP-ready climate information.

### 3.3. Soils Data

WEPP requires detailed soil properties (e.g., texture, organic matter, hydraulic conductivity) to model infiltration and erodibility. The premier source for this information in the US is the SSURGO database.

* **Primary Dataset:** **Gridded Soil Survey Geographic (gSSURGO) Database**. This is a rasterized version of the most detailed soils data available from the NRCS.
* **Acquisition Portal:** **NRCS Geospatial Data Gateway**.
    * *Direct Link:* [https://datagateway.nrcs.usda.gov/](https://datagateway.nrcs.usda.gov/)
* **Selection Rationale:** gSSURGO provides the highest-resolution, most comprehensive soil property data available nationally. While WEPP requires specific formatting (`.sol` files), the raw gSSURGO data contains all the necessary attributes (e.g., sand/silt/clay percentages, organic matter, rock fragment content) that will be processed in a subsequent task (Weeks 2-3) to create these files.

## 4. Implementation Protocol (Jupyter Notebook Workflow)

This protocol will be implemented in a dedicated Jupyter Notebook (`02_acquire_raw_data.ipynb`).

### Step 1: Load Study Area Polygons
Load the `study_areas.gpkg` and `validation_area.gpkg` files created in the previous task. Combine them into a single GeoDataFrame representing the total data acquisition extent.

### Step 2: Download DEM Data
Automate the download of 1/3 arc-second DEM data from The National Map. This can be achieved using a Python library that interfaces with the TNM API or by providing clear, documented instructions for manual download using the web interface.
* Define the total extent of all study polygons as the Area of Interest (AOI).
* Select "Elevation Products (3DEP)" and the "1/3 arc-second DEM" dataset.
* Download the required tiles in GeoTIFF format.

### Step 3: Download Climate Data
Using the Rock:Clime web interface:
* Navigate to the locations of your study areas.
* Download the `.par` (parameter) and associated climate station files for a representative set of points within each study polygon. The goal is to capture the climatic variability across the study areas.
* Document the station IDs and locations selected.

### Step 4: Download Soils Data
Using the Geospatial Data Gateway:
* Select your state(s) of interest.
* Navigate to the "Soil Survey (SSURGO/STATSGO2)" data products.
* Order the complete gSSURGO database for the relevant state(s). This is typically provided as a link to download a ZIP file containing the raster data (e.g., in File Geodatabase format) and associated tabular data.

### Step 5: Organize Raw Data
Organize all downloaded files into a clear, consistent directory structure within the project's `data/raw/` directory.

data/
└── raw/
├── dem/
│   ├── tile1.tif
│   └── tile2.tif
├── climate/
│   ├── station_ID1.par
│   └── ...
└── soils/
└── gSSURGO_State/
└── ...

### Step 6: Data Ingestion and Clipping (Initial Processing)
* **DEMs:** Create a virtual raster (VRT) from all downloaded DEM tiles to treat them as a single mosaic. Clip this mosaic to the buffered extent of the study area polygons.
* **Soils:** Load the key gSSURGO raster layers (e.g., `MapunitRaster_10m`). Clip them to the same extent as the DEMs.
* Save these initial processed files to a new directory: `data/interim/`.

## 5. Deliverables

1.  **Raw Data Files:** All original, unmodified downloaded files stored in the `data/raw/` project directory, organized by data type.
2.  **Clipped Interim Datasets:**
    * `data/interim/dem_mosaic_clipped.tif`: The clipped mosaic of all 3DEP DEM tiles.
    * `data/interim/soils_mapunit_clipped.tif`: The clipped gSSURGO map unit raster.
3.  **Jupyter Notebook (`02_acquire_raw_data.ipynb`)**: A fully executed notebook documenting the data sources, download parameters (e.g., AOI, dates), and the code used for initial clipping and processing. The notebook should include a markdown cell summarizing the downloaded files and their on-disk locations.

## 6. Quality Control Checklist

* [ ] All three data types (DEM, climate, soils) have been successfully downloaded.
* [ ] Data sources match those specified in the protocol.
* [ ] Raw files are organized correctly within the `data/raw/` directory structure.
* [ ] A VRT has been created for the DEM tiles.
* [ ] DEM and soils rasters have been clipped to the full study and validation area extent.
* [ ] All download and initial processing steps are documented and reproducible in the Jupyter Notebook.