{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1.3: WEPP Climate (.cli) File Preparation\n",
    "\n",
    "**Objective:** To process the raw daily PRISM climate rasters (precipitation, min/max temperature) downloaded in the previous notebook and generate a WEPP-compatible climate file (`.cli`) for each study area.\n",
    "\n",
    "**Gold-Standard Practices Implemented:**\n",
    "- **Configuration-Driven:** All paths and parameters are loaded from the central `config.yml`.\n",
    "- **Spatial Averaging:** For each polygon in our study areas, we will extract the climate data and calculate a single, representative daily time series.\n",
    "- **Custom Formatting:** A dedicated function will format the time series data into the precise, column-based ASCII format required by the WEPP model.\n",
    "- **Idempotent:** The notebook is designed to be run multiple times without re-generating existing `.cli` files.\n"
   ],
   "id": "51885cbd1e8acc59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T22:46:38.186290Z",
     "start_time": "2025-10-03T22:46:36.580735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 1. Configuration & Setup ===\n",
    "\n",
    "# --- Core Libraries ---\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Project-Specific Modules ---\n",
    "# Add project's src directory to path to allow imports\n",
    "def find_project_root(marker='config.yml'):\n",
    "    path = Path.cwd().resolve()\n",
    "    while path.parent != path:\n",
    "        if (path / marker).exists(): return path\n",
    "        path = path.parent\n",
    "    raise FileNotFoundError(f\"Project root with marker '{marker}' not found.\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT / 'src'))\n",
    "\n",
    "from utils import setup_colored_logging\n",
    "\n",
    "# --- Geospatial & Data Libraries ---\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rioxarray\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Gold-Standard Logging Setup ---\n",
    "setup_colored_logging()\n",
    "log = logging.getLogger(\"1.3_wepp_climate_preparation\")\n",
    "\n",
    "# --- Configuration Loading ---\n",
    "CONFIG_PATH = PROJECT_ROOT / \"config.yml\"\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# --- Path Configuration (from config) ---\n",
    "STUDY_AREAS_GPKG = PROJECT_ROOT / config['paths']['study_areas']\n",
    "RAW_CLIMATE_DIR = PROJECT_ROOT / config['paths']['climate_dir'] / 'daily'\n",
    "PROCESSED_CLIMATE_DIR = PROJECT_ROOT / config['paths']['processed_dir'] / 'climate_cli'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "PROCESSED_CLIMATE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Parameter Configuration ---\n",
    "WGS84_CRS = config['parameters']['wgs84_crs']\n",
    "PRISM_DATE_RANGE = config['data_sources']['prism']['date_range']\n",
    "\n",
    "log.info(\"--- Configuration Summary ---\")\n",
    "log.info(f\"Project Root:          {PROJECT_ROOT}\")\n",
    "log.info(f\"Input Study Areas:     {STUDY_AREAS_GPKG}\")\n",
    "log.info(f\"Input Raw Climate:     {RAW_CLIMATE_DIR}\")\n",
    "log.info(f\"Output .cli Files:     {PROCESSED_CLIMATE_DIR}\")\n",
    "log.info(\"Setup complete.\")\n"
   ],
   "id": "c2266c43894f4aea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;20m2025-10-03 22:46:38 - 1.3_wepp_climate_preparation - INFO - --- Configuration Summary ---\u001B[0m\n",
      "\u001B[38;20m2025-10-03 22:46:38 - 1.3_wepp_climate_preparation - INFO - Project Root:          /workspace\u001B[0m\n",
      "\u001B[38;20m2025-10-03 22:46:38 - 1.3_wepp_climate_preparation - INFO - Input Study Areas:     /workspace/data/processed/study_areas.gpkg\u001B[0m\n",
      "\u001B[38;20m2025-10-03 22:46:38 - 1.3_wepp_climate_preparation - INFO - Input Raw Climate:     /workspace/data/raw/climate_prism/daily\u001B[0m\n",
      "\u001B[38;20m2025-10-03 22:46:38 - 1.3_wepp_climate_preparation - INFO - Output .cli Files:     /workspace/data/processed/climate_cli\u001B[0m\n",
      "\u001B[38;20m2025-10-03 22:46:38 - 1.3_wepp_climate_preparation - INFO - Setup complete.\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === 2. Load Study Area Polygons ===\n",
    "\n",
    "def load_study_areas(gpkg_path: Path) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Loads the study area polygons that will be used to generate climate files.\"\"\"\n",
    "    log.info(f\"Loading study area polygons from {gpkg_path}\")\n",
    "    if not gpkg_path.exists():\n",
    "        raise FileNotFoundError(f\"Study areas file not found at {gpkg_path}. Please run notebook 1.1 first.\")\n",
    "    \n",
    "    # Assuming the relevant layer is 'cv_provinces' for this example\n",
    "    # This might need to be adjusted to point to specific watershed/hillslope polygons later\n",
    "    gdf = gpd.read_file(gpkg_path, layer='cv_provinces')\n",
    "    log.info(f\"Loaded {len(gdf)} study area polygons.\")\n",
    "    return gdf\n",
    "\n",
    "# --- Execute ---\n",
    "study_areas_gdf = load_study_areas(STUDY_AREAS_GPKG)\n",
    "display(study_areas_gdf.head())\n"
   ],
   "id": "182a79c0b38d5839"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Climate Data Extraction Strategy\n",
    "\n",
    "For each study area polygon, we need to generate a single representative time series. The process is as follows:\n",
    "\n",
    "1.  **Identify Relevant Files:** For each day in our period of record, we locate the corresponding raw PRISM raster for precipitation, minimum temperature, and maximum temperature.\n",
    "2.  **Clip and Aggregate:** For a given study area polygon, we will use `rioxarray` to clip the global daily raster to the polygon's bounds. \n",
    "3.  **Calculate Spatial Mean:** After clipping, we will calculate the mean value of all the pixels within the polygon. This gives us a single value (e.g., mean precipitation) for that specific day and that specific study area.\n",
    "4.  **Build Time Series:** We repeat this for every day in the date range, building a complete `pandas` DataFrame that represents the daily climate for the study area.\n",
    "\n",
    "This method ensures that the climate data is accurately tailored to the specific geographic boundaries of each analysis unit.\n"
   ],
   "id": "7906dc15c7bea94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === 3. Core Climate Processing Logic ===\n",
    "\n",
    "# This is a placeholder for the core data processing logic.\n",
    "# A full implementation would involve:\n",
    "# 1. Iterating through each polygon in `study_areas_gdf`.\n",
    "# 2. For each polygon, creating a date range from the config.\n",
    "# 3. For each date, opening the corresponding ppt, tmin, and tmax files.\n",
    "# 4. Clipping the raster with the polygon's geometry.\n",
    "# 5. Calculating the mean of the clipped raster.\n",
    "# 6. Storing the results in a pandas DataFrame.\n",
    "# 7. Passing the completed DataFrame to a formatting function (defined in the next cell).\n",
    "\n",
    "log.warning(\"Placeholder cell: Core data extraction logic needs to be implemented.\")\n",
    "\n",
    "# Example structure:\n",
    "# for index, area in study_areas_gdf.iterrows():\n",
    "#     area_id = area['some_unique_id'] # e.g., province name or watershed ID\n",
    "#     log.info(f\"Processing climate data for {area_id}...\")\n",
    "#     \n",
    "#     # Check if final .cli file already exists\n",
    "#     cli_output_path = PROCESSED_CLIMATE_DIR / f\"{area_id}.cli\"\n",
    "#     if cli_output_path.exists():\n",
    "#         log.info(f\"  -> Skipping, {cli_output_path.name} already exists.\")\n",
    "#         continue\n",
    "#\n",
    "#     # ... implementation of steps 1-6 ...\n",
    "#     daily_climate_df = ...\n",
    "#\n",
    "#     # ... call formatter ...\n",
    "#     write_wepp_cli_file(daily_climate_df, cli_output_path, area)\n",
    "\n"
   ],
   "id": "4c5044cd2ea6553e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### WEPP `.cli` File Format\n",
    "\n",
    "The WEPP model requires a very specific ASCII text file format for its climate input. The file is space-delimited and has a header providing metadata about the station. The main body contains daily records with the following columns:\n",
    "\n",
    "-   `Day`\n",
    "-   `Month`\n",
    "-   `Year`\n",
    "-   `Precipitation` (mm)\n",
    "-   `Duration` (hours) - *Often estimated*\n",
    "-   `Peak Intensity` (mm/hr) - *Often estimated*\n",
    "-   `Max Temperature` (°C)\n",
    "-   `Min Temperature` (°C)\n",
    "-   `Solar Radiation` (Langleys/day) - *May need to be estimated*\n",
    "-   `Wind Velocity` (m/s) - *May need to be estimated*\n",
    "-   `Wind Direction` (degrees) - *Often set to 0 if not available*\n",
    "\n",
    "Our initial implementation will focus on formatting the data we have (Precip, Tmax, Tmin) and using reasonable defaults or estimation methods for the other required parameters.\n"
   ],
   "id": "873df1bcf3bfd2bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === 4. WEPP .cli Formatter ===\n",
    "\n",
    "def write_wepp_cli_file(daily_data: pd.DataFrame, out_path: Path, area_info: pd.Series):\n",
    "    \"\"\"\n",
    "    Formats a DataFrame of daily climate data into a WEPP .cli file.\n",
    "\n",
    "    Args:\n",
    "        daily_data (pd.DataFrame): Must have columns 'date', 'ppt_mm', 'tmax_c', 'tmin_c'.\n",
    "        out_path (Path): The full path for the output .cli file.\n",
    "        area_info (pd.Series): GeoDataFrame row containing metadata (lat/lon, name).\n",
    "    \"\"\"\n",
    "    log.info(f\"Generating WEPP .cli file: {out_path.name}\")\n",
    "\n",
    "    # --- Placeholder for Estimating Missing Parameters ---\n",
    "    # WEPP requires radiation, wind, duration, etc.\n",
    "    # For this example, we will use placeholder values.\n",
    "    # A robust implementation would use an estimation library or empirical formulas.\n",
    "    daily_data['rad_ly'] = 250  # Placeholder for Solar Radiation (Langleys)\n",
    "    daily_data['wind_vel_ms'] = 2    # Placeholder for Wind Velocity (m/s)\n",
    "    daily_data['wind_dir_deg'] = 0    # Placeholder for Wind Direction\n",
    "    daily_data['dur_hr'] = daily_data['ppt_mm'].apply(lambda x: 1.0 if x > 0 else 0.0) # Simple duration\n",
    "    daily_data['ip_mm_hr'] = daily_data['ppt_mm'] / daily_data['dur_hr'].replace(0, 1) # Simple peak intensity\n",
    "\n",
    "    # --- Get metadata for the header ---\n",
    "    # Calculate representative lat/lon from the polygon's centroid\n",
    "    centroid = area_info.geometry.centroid\n",
    "    lat, lon = centroid.y, centroid.x\n",
    "    station_name = area_info.get('Name', 'Unknown') # Get name from a column if it exists\n",
    "    num_years = len(daily_data['date'].dt.year.unique())\n",
    "\n",
    "    # --- Write the file ---\n",
    "    with open(out_path, 'w') as f:\n",
    "        # --- Write Header ---\n",
    "        f.write(\"WEPPCLI v1.0\\n\")\n",
    "        f.write(\"#\\n\")\n",
    "        f.write(f\"# Climate file generated by pyWEPP on {pd.Timestamp.now().strftime('%Y-%m-%d')}\\n\")\n",
    "        f.write(f\"# Station: {station_name}\\n\")\n",
    "        f.write(\"#\\n\")\n",
    "        f.write(\"1 1\\n\") # Agent number, version\n",
    "        f.write(f\"{lat:.4f} {lon:.4f} 100.0\\n\") # Lat, Lon, Elevation (placeholder)\n",
    "        f.write(f\"{num_years} 0\\n\") # Number of years, 0\n",
    "\n",
    "        # --- Write Daily Data ---\n",
    "        for row in daily_data.itertuples():\n",
    "            line = (\n",
    "                f\"{row.date.day:>4} {row.date.month:>4} {row.date.year:>6} \"\n",
    "                f\"{row.ppt_mm:8.2f} {row.dur_hr:8.2f} {row.ip_mm_hr:8.2f} \"\n",
    "                f\"{row.tmax_c:8.2f} {row.tmin_c:8.2f} {row.rad_ly:8.2f} \"\n",
    "                f\"{row.wind_vel_ms:8.2f} {row.wind_dir_deg:8.2f}\\n\"\n",
    "            )\n",
    "            f.write(line)\n",
    "            \n",
    "    log.info(f\"✅ Successfully wrote {out_path.name}\")\n",
    "\n",
    "log.warning(\"Placeholder cell: Formatting function is defined but not yet called.\")"
   ],
   "id": "b570ba14c390d152"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
