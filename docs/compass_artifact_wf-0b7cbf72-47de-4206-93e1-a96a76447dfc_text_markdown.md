# Addressing TDA Research Critiques for Geomorphic Classification

Your research proposal using Topological Data Analysis for geomorphic classification and soil organic carbon prediction in the Appalachians can be strengthened significantly through recent methodological advances in spatial validation, multi-scale analysis, and legacy effects quantification. The most critical gaps to address are **geographic transferability testing using Area of Applicability methods, incorporation of historical land use data from the HYDE database, and systematic multi-scale window testing from 250m to 4km**. These three elements will transform your proposal from a novel computational approach into a robust, defensible framework that addresses reviewer concerns about spatial extrapolation, anthropogenic signal separation, and scale dependency.

The literature reveals that random cross-validation can underestimate prediction errors by 5-54% for spatially autocorrelated data, making spatial validation frameworks essential. Meanwhile, the Piedmont region lost an average of 17 cm of topsoil between 1700-1970, creating legacy sediment deposits that fundamentally altered the topographic signal your TDA methods will detect. Addressing these issues requires specific methodological choices detailed below, organized by the discipline-specific solutions you requested.

## Geography solutions provide the strongest foundation for addressing transferability and validation concerns

The Area of Applicability framework developed by Meyer and Pebesma provides the methodological foundation for addressing geographic transferability critiques. Published in Methods in Ecology and Evolution in 2021, this approach calculates a Dissimilarity Index based on minimum distance to training data in multidimensional predictor space, weighted by variable importance. The **AOA defines where your model can reliably extrapolate** by identifying regions where the feature space matches the training data. Implementation through the CAST R package is straightforward, with functions for AOA calculation, spatial cross-validation, and forward feature selection integrated with the caret machine learning framework.

For multi-region testing within CONUS, the most defensible approach involves stratifying by physiographic provinces using the USGS Fenneman classification system. Your Appalachian study area encompasses seven distinct provinces including Piedmont, Blue Ridge, Valley and Ridge, and Appalachian Plateaus. The literature demonstrates that training models in one region and testing in others with varying environmental similarity reveals transferability limitations. A recent study comparing Kraichgau and Swabian Alb regions in Germany found that within-region R² of 0.33-0.64 collapsed to near-zero when transferred just 100 km away due to different soil-landscape relationships. For your research, this suggests testing transferability between the Appalachians and contrasting regions like the Rocky Mountains (high contrast) or within-province comparisons (low contrast) to establish confidence bounds.

The spatial cross-validation framework requires careful buffer distance selection based on spatial autocorrelation ranges. The blockCV R package provides automated variogram fitting across multiple covariates to determine appropriate blocking distances. For soil properties, **typical autocorrelation ranges span 100-500m for agricultural fields and 500-5000m for parent material effects**. The package's cv_spatial_autocor function samples 5000 random points from continuous rasters, fits variogram models automatically, and returns the median range as a suggested block size. Your TDA features computed at 250m-4km windows will have different autocorrelation structures than the underlying DEM, necessitating separate variogram analysis for each scale. The recent k-fold Nearest Neighbour Distance Matching approach by Milà and Meyer (2022) provides computationally efficient spatial CV that matches the distribution of distances between training and test data, ensuring your validation mimics real prediction scenarios.

Legacy land use effects represent a critical but often-overlooked source of topographic variation in the Appalachians. Trimble's foundational work documented that the Piedmont experienced erosion rate increases from 0.008 mm/yr pre-settlement to 0.95 mm/yr during peak agricultural activity (1860-1920), representing a 100-fold acceleration. Remarkably, **upwards of 90% of historically eroded soil remains stored in hillslopes and valley floodplains today**, fundamentally altering the topographic signal. Walter and Merritts' 2008 Science paper revealed that most mid-Atlantic floodplains are actually fill terraces from 1-5 meters of slackwater sedimentation behind tens of thousands of 17th-19th century milldams. This anthropogenic topography will appear in your DEM data and produce topological features unrelated to natural geomorphic processes.

The HYDE database provides global land use reconstructions from 10,000 BCE to 2025 CE at 5 arc-minute resolution, including cropland coverage, grazing lands, and population density. For the Southern Piedmont, cropland coverage increased from 2.2% in 1700 CE to 4.4% by 1850 CE and 12.2% by 2015 CE. Sanderman's 2017 PNAS study demonstrated how to incorporate HYDE data as covariates in machine learning models, using quantile regression forests fitted with historical land use combined with climatic, landform, and lithology covariates. This approach estimated global agricultural soil carbon debt at 133 Pg C. For your research, incorporating HYDE land use variables (cropland fraction, grazing intensity, years since abandonment) along with topographic position indicators (summit, shoulder, footslope) will enable your models to separate anthropogenic from natural topographic signals.

Spatial error analysis beyond simple accuracy metrics provides critical insights into where and why models fail. Ploton's 2020 Nature Communications study demonstrated that random 10-fold cross-validation suggested R² of 0.53, while spatial 44-fold CV revealed actual performance of only R² of 0.14 for forest biomass prediction. Their Buffered Leave-One-Out approach tested increasing buffer radii from 0-150 km, showing R² decline from 0.50 at 0 km to near-zero beyond 100 km. This distance-dependent performance assessment reveals the effective prediction range of your models. For your TDA approach, mapping prediction residuals spatially and analyzing their autocorrelation structure will identify systematic over-prediction or under-prediction in specific physiographic settings. If residuals show strong spatial structure with Nugget-to-Sill ratios below 0.3, kriging of residuals can marginally improve predictions, though at global scales most variograms show pure nugget effects.

## Computer science solutions focus on existing libraries rather than reinventing computational wheels

GPU acceleration for persistent homology shows promise but has critical limitations for your application. Ripser++ achieves up to 30x speedup over CPU Ripser for Vietoris-Rips complexes, but the core persistence computation remains CPU-based with GPU primarily accelerating filtration construction. More importantly for terrain analysis, **CubicalRipser is the fastest implementation for 2D/3D cubical complexes from DEMs**, and it is CPU-only. The highly optimized cache control and V-construction (4-neighbor) or T-construction (8-neighbor) options make it ideal for DEM data. Installation via pip install cripser provides a Python interface that integrates with standard geospatial workflows. GUDHI, despite common misconceptions, has no native CUDA support. Its TensorFlow integration enables automatic differentiation for machine learning applications, but documentation explicitly states that "even if TensorFlow GPU is enabled, all internal computations using Gudhi will be done on CPU."

For processing CONUS-scale 10m DEMs, the computational challenge stems from the ~280 billion pixels (700,000 × 400,000 grid). Cloud-Optimized GeoTIFF format solves the data access bottleneck through internal tiling (typically 512×512 or 1024×1024 pixels), embedded overviews, and HTTP range request support. **USGS converted all elevation data to COG in December 2019**, improving processing time and storage efficiency. Creating COGs with DEFLATE compression and predictor=2 (optimal for elevation data) reduces raw Int16 storage from ~560 GB to 200-300 GB while maintaining lossless compression. The internal tile structure enables parallel processing without loading entire datasets into memory.

Dask provides the Python ecosystem for distributed raster processing through the rioxarray library. Opening COGs with appropriate chunking (1, 6144, 6144) creates lazy Dask arrays that compute only when needed. Setting lock=False enables true parallelism since COGs are read-only and thread-safe. The key challenge is determining optimal chunk sizes, with the general recommendation being ~100 MB per chunk. For CONUS 10m processing, **6144×6144 chunks represent approximately 72 MB for Int16 data**, balancing parallel efficiency against Dask overhead. Chunks smaller than 4 MB create excessive coordination overhead, while chunks too large reduce parallelization opportunities.

The tile-based processing workflow for persistent homology involves decomposing CONUS into roughly 12,000 tiles of 6144×6144 pixels each, processing them in parallel, and merging results. A critical consideration is overlap handling, with 512-pixel (5.12 km) overlap recommended between tiles to prevent boundary artifacts. CubicalRipser processing time for individual tiles ranges from 1-10 seconds depending on topographic complexity, yielding estimated full CONUS completion in 0.5-2 hours on a 64-node cluster. The bottleneck shifts from computation to feature stitching across tile boundaries, where persistence pairs must be matched and merged using spatial proximity and birth-death value similarity criteria.

Software engineering best practices from the TorchGeo and GeoAI projects provide templates for packaging your research. TorchGeo, published in ACM Transactions on Spatial Algorithms and Systems in 2024, demonstrates production-quality geospatial ML with PyTorch integration, pre-trained models for multispectral imagery, and CI/CD testing across Linux, macOS, and Windows. The project's GitHub repository shows pytest-based testing frameworks, Docker containerization for reproducibility, and comprehensive documentation. For your TDA implementation, following similar patterns with automated testing (pytest for unit tests, integration tests for full workflows), version-controlled environments (conda/pip requirements.txt), and containerization (Docker with GDAL, rasterio, GUDHI dependencies) ensures reproducibility within the 3-year PhD timeline.

High-performance computing benchmarking requires reporting both strong and weak scaling results. Strong scaling tests fixed problem size with increasing processors, measuring speedup S(N) = T(1)/T(N) and efficiency E(N) = T(1)/(N × T(N)). Ideal linear speedup shows E(N) = 1.0, though communication overhead typically reduces efficiency as processor count increases. Weak scaling maintains constant workload per processor as problem size grows, testing whether runtime remains stable. For geospatial algorithms, Hoefler and Belli's 2015 SC paper "Scientific benchmarking of parallel computing systems" provides the methodological framework, emphasizing statistically sound analysis with confidence intervals rather than single best-case runs. Documenting hardware specifications, network topology, and testing across different HPC architectures prevents the "twelve ways to fool the masses" pitfalls identified in the parallel computing literature.

## Geology and geomorphometry solutions address feature interpretation and baseline comparisons

Feature redundancy testing using Variance Inflation Factor analysis identifies multicollinear terrain metrics that provide no incremental predictive value. VIF measures how much the variance of a regression coefficient increases due to collinearity, calculated as VIF_i = 1/(1 - R²_i) where R²_i comes from regressing predictor i on all other predictors. Standard interpretation guidelines suggest **VIF values above 5 indicate potentially problematic correlation, while VIF above 10 requires attention**. For terrain metrics, slope, relief, and roughness measures often show high intercorrelation (VIF 8-15), while curvature variants, Topographic Wetness Index, and Topographic Position Index demonstrate lower correlations. The car R package's vif function automatically computes Generalized VIF for models with categorical variables like lithology, handling multi-degree-of-freedom terms correctly.

SHAP values provide feature importance analysis with both local (instance-level) and global (aggregated) interpretations grounded in game theory. TreeSHAP for tree-based models like random forests computes efficiently, while KernelSHAP handles arbitrary model types at greater computational cost. A 2024 soil temperature prediction study in Frontiers in Environmental Science used SHAP to identify that air temperature at 2m had greatest influence with significant seasonality in SHAP values. For your TDA features, SHAP analysis will reveal whether persistence values from H0 (connected components), H1 (loops), or specific scale windows contribute most to SOC predictions. The shap Python package provides summary plots showing both magnitude and direction of effects, dependence plots revealing interaction patterns, and force plots explaining individual predictions.

Comparing novel TDA metrics against traditional terrain attributes requires systematic incremental value testing. A 2025 Science of the Total Environment study found that a comprehensive terrain complexity index integrating fractal dimension, entropy, rugosity, volume filling ratio, and slope outperformed individual features by 2-10% for soil water erosion and plant diversity predictions. Individual terrain features explained 7-21% of variance, while the integrated index provided superior predictive capability. For your research, this suggests first establishing baseline model performance using conventional metrics (slope, aspect, curvature, TWI, TPI), then adding TDA features incrementally. If TDA features improve R² by less than 5% after accounting for computational cost, the practical value becomes questionable. Conversely, if TDA features capture unique aspects of topographic organization not reflected in traditional metrics, improvements of 10-20% justify the methodological complexity.

The USGS State Geologic Map Compilation provides authoritative lithology data for CONUS at 1:100,000 to 1:500,000 scale. The 2017 Data Series 1052 compilation includes standardized lithology classifications, geologic age, and comprehensive GIS structure with five supplemental attribute tables. Downloadable from mrdata.usgs.gov/geology/state/, the database uses dominant and secondary lithology fields with rock type codes based on mineralogical and chemical composition. **Gray's 2015 Catena study demonstrated that lithology had the highest influence of all covariates for all six soil properties tested at the NSW scale**, with silica index showing a 22% relative decrease in SOC with each unit increase. This suggests that excluding lithology from your models risks omitting the dominant control on soil properties, potentially attributing lithology effects incorrectly to topographic features.

Encoding categorical lithology variables for machine learning requires different strategies by algorithm type. For tree-based models like random forests, one-hot encoding or simple label encoding work effectively since trees can handle categorical splits naturally. For linear models, creating N-1 dummy variables avoids multicollinearity, with the reference category absorbed into the intercept. Target encoding replaces categories with the mean target value for that category, useful for high-cardinality lithology data but requiring cross-validation to prevent overfitting. Gray's silica index approach provides an elegant alternative, creating a continuous geochemical gradient from ultramafic through mafic, intermediate, and felsic igneous rocks, plus calcareous versus siliciclastic sedimentary classes. This reduces dimensionality while preserving pedologically meaningful variation in parent material chemistry that controls weathering rates and nutrient availability.

Geomorphon analysis provides an accessible baseline for topological-geomorphic interpretation without requiring advanced computational topology. The pattern recognition approach by Jasiewicz and Stepinski (2013) classifies terrain into ten standard landforms (flat, peak, ridge, shoulder, spur, slope, hollow, footslope, valley, pit) based on Local Ternary Patterns using line-of-sight comparisons in eight directions. Implemented in GRASS GIS as r.geomorphon and available in ArcGIS Pro since version 3.1, the method is relief-invariant, orientation-invariant, and self-adapting in spatial scale. Validation studies report **70%+ accuracy for coastal plains and drainage networks, though 20-30% lower for complex terraces**. The 498 unique geomorphon patterns possible provide finer classification than the standard ten types, enabling custom landform taxonomies. Comparing your TDA-derived critical points (peaks, pits, saddles) against geomorphon classifications provides visual validation of whether topological features align with conventional geomorphological understanding.

Morse-Smale complexes provide rigorous topological foundations for terrain analysis, decomposing surfaces into regions based on gradient flow to critical points. The MSCEER software provides shared-memory parallel computation with improved accuracy through multiple discrete gradient algorithms. Index 0 critical points (minima) correspond to pits and valley bottoms, index 1 saddles mark passes and connections between valleys and ridges, while index 2 maxima identify peaks and summits. The Topology ToolKit provides ParaView integration for 3D visualization with topological overlays, rendering critical points as spheres and enabling split views comparing simplified versus original data. For your research, Morse-Smale analysis of the same DEM data used for TDA provides a comparison framework, testing whether persistence-based feature selection identifies the same critical terrain elements as gradient-based topology.

## Novel research directions emerge from integrating these disciplinary perspectives

The synthesis of geographic transferability testing, legacy effects quantification, and topological feature extraction suggests research questions beyond the original proposal scope. If anthropogenic topography from legacy sediments creates persistent homology features (loops, voids) that differ systematically from natural geomorphic patterns, could TDA serve as an automated detector of human landscape modification? The characteristic scale of milldam-induced sedimentation (1-5 meters vertical, 50-500 meters horizontal) might produce H1 persistence signatures at specific spatial scales. Testing this hypothesis requires comparing TDA features between heavily impacted Piedmont watersheds and relatively pristine mountain catchments with minimal historical agriculture.

The scale dependency of optimal terrain metrics presents opportunities for multi-scale TDA ensembles. Rather than selecting a single window size based on preliminary testing, computing persistent homology across the full 250m-4km range and using scale-dependent features as a high-dimensional descriptor captures hierarchical landscape organization. Random forest variable importance analysis could reveal whether SOC variation is controlled by local-scale (250-500m) topographic complexity in steep mountain terrain versus regional-scale (2-4km) landscape position in lower-relief Piedmont settings. This would formalize the geomorphometric principle that optimal scale varies by physiographic setting.

The computational pipeline for validating TDA-based predictions should prioritize efficiency over exhaustive analysis. Begin with prototype development on a single HUC8 watershed (~2000 km²) representing each major physiographic province. Use CubicalRipser for rapid persistent homology computation, implement blockCV for spatial cross-validation with variogram-determined buffer distances, and incorporate HYDE historical cropland as a covariate. This phase establishes baseline performance and identifies methodological issues before scaling. State-level expansion (500-1000 tiles) tests distributed processing workflows and tile-stitching algorithms. CONUS-scale deployment (12,000 tiles) occurs only after regional validation confirms the approach generalizes across environmental gradients. This staged approach prevents premature investment in computational infrastructure before methodological validation.

The most significant research contribution may be methodological rather than applied: demonstrating how to rigorously validate novel feature engineering approaches in spatial prediction contexts. The combination of Area of Applicability analysis, spatially-blocked cross-validation, SHAP-based feature importance, and incremental predictive value testing against baseline methods creates a validation framework applicable beyond TDA to any proposed terrain metric. Publishing the complete workflow with containerized environments, documented datasets, and reproducible analysis scripts through platforms like Zenodo or GitHub enables future researchers to adapt the validation framework for their own methods. This meta-contribution addresses the broader challenge in digital soil mapping and geomorphometry: distinguishing truly novel insights from spurious patterns arising from spatial autocorrelation and overfitting.

The three-year PhD timeline constrains both geographic scope and computational experimentation. Prioritizing the Appalachian region with its exceptional historical documentation of land use change and legacy sediment research creates a coherent narrative connecting TDA methodology to specific geomorphological questions. The regional focus enables leveraging existing high-resolution LiDAR (often 1-3m posting), detailed soil survey data from SSURGO, and published legacy sediment studies identifying anthropogenic deposits. Attempting CONUS-wide analysis without first establishing regional success risks spreading effort too thinly across data acquisition, computational infrastructure development, and methodological validation. The most impactful dissertation will deeply address transferability and validation questions in a well-characterized region rather than superficially mapping the entire continent.