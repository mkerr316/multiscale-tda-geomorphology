# Validating Topological Data Analysis for Terrain Classification: A Methodological Framework

Topological Data Analysis (TDA) applications to terrain data require rigorous validation protocols that go far beyond standard machine learning practices. The critical challenge is proving that persistent homology features represent real geomorphological phenomena rather than computational artifacts, while accounting for spatial autocorrelation that can inflate accuracy estimates by **28-50%**. Recent validation studies of TDA for landslide detection achieved 70-96% accuracy, but only when integrating topological features with geometrical and contextual information—topological features alone produced unacceptably high false positive rates. The field has established that spatial cross-validation with blocks exceeding 1.5× the autocorrelation range is mandatory, not optional, as demonstrated by studies where random cross-validation yielded R²=0.53 but spatial cross-validation revealed the true predictive power of just R²=0.14. This framework synthesizes validation standards across five domains: TDA-specific validation protocols, Appalachian physiographic province classification as a concrete application, baseline geomorphometry methods for comparison, recent literature on persistent homology for DEMs, and statistical best practices for spatial data.

## TDA validation requires multi-layered verification that traditional methods cannot provide

Validating TDA applications to Digital Elevation Models demands a fundamentally different approach than standard terrain classification because topological features can emerge from computational choices rather than geological reality. The theoretical foundation comes from the **stability theorem of Cohen-Steiner et al. (2007)**, which guarantees that for tame functions, the bottleneck distance between persistence diagrams is bounded by the sup-norm difference between functions: W∞(D(f), D(g)) ≤ ||f - g||∞. This provides the mathematical justification that small perturbations in elevation values cause bounded changes in topology, but does not distinguish meaningful geological features from preprocessing artifacts.

The most successful validation approach combines multiple independent verification streams. Ground truth comparison against existing inventories forms the primary validation: Syzdykbayev et al. (2020) validated landslide detection from LiDAR DTMs by comparing against USGS landslide inventories across five study areas, achieving accuracies of 0.53 to 0.79. However, this revealed that performance varied substantially by surface roughness, landslide age and composition, and topographic complexity—establishing that TDA sensitivity depends on domain-specific terrain characteristics that must be systematically documented. The landmark Bhuyan et al. (2024) study in Nature Communications demonstrated robust transferability by training on **250,000 Italian landslide samples** (using 54,440 for balanced training) and testing across geographically diverse regions: US Pacific Northwest (47,653 landslides), Denmark (3,202), Turkey (900), and multi-temporal data from Wenchuan, China (2005-2018). Their geographic transferability testing achieved 80-94% accuracy across regions with vastly different geology, climate, and tectonic settings, while temporal transferability exceeded 90% accuracy, proving the method captured fundamental topological signatures rather than location-specific artifacts.

Distinguishing real geological features from computational artifacts requires persistence-based discrimination combined with statistical significance testing. Features with high persistence (large death-birth difference in persistence diagrams) are more likely to represent genuine terrain structures, while features near the diagonal typically represent noise. Bobrowski and Skraba (2023) developed a parametric approach where each topological feature is tested against a left-skewed Gumbel distribution with Bonferroni correction (testing each p-value against α/ni where ni is the number of features), though this proves very conservative in practice. More robust is the multi-sample validation approach of Chazal et al. (2015): drawing multiple subsamples, computing persistence landscapes for each, and accepting only features that appear consistently across subsamples. Robinson and Turner (2017) formalized hypothesis testing by generating null distributions through permutation tests, rejecting features that don't significantly differ from random patterns.

The critical limitation revealed by Syzdykbayev et al. (2024) fundamentally changes TDA application strategy: **topological information alone produces notable false positives**. Their knowledge-based method required integration with geometrical filters (area, perimeter, convexity) and contextual information (slope, vegetation indices, soil moisture) to achieve acceptable accuracy. This establishes the principle that TDA should never be applied in isolation for terrain analysis—it must be combined with conventional terrain attributes. Bhuyan et al. (2024) quantified this directly: their six optimal topological features achieved **94% classification accuracy**, while seven geometric features achieved only ~65%, but the integrated approach proved essential for geographic transferability.

Preprocessing sensitivity analysis represents a mandatory validation component often neglected in TDA applications. Resolution sensitivity must be tested systematically across multiple grid resolutions (minimum five different resolutions with logarithmic spacing, typically rᵢ₊₁/rᵢ ≈ 1.5-2.0) because different scales reveal different topological features—fine resolution captures local landforms while coarse resolution reveals regional structures. The Distance-to-Measure (DTM) approach provides robustness to noise through the formula dP,m,r(x) = (1/m ∫₀ᵐ δᵤP,ᵤ(x)ʳ du)^(1/r), which is Wr-Lipschitz stable with respect to Wasserstein distance and controlled by parameter m ∈ [0,1] for outlier tolerance. Filtration parameter sensitivity demands testing multiple ε-neighborhood radii, bandwidth values for kernel density estimation (typically 0.5-2.0× estimated bandwidth), and threshold values for sublevel set filtrations. The choice of complex construction matters substantially: Čech complexes are theoretically optimal but computationally expensive, Vietoris-Rips complexes provide practical alternatives, and alpha complexes optimize for Euclidean data, with each producing potentially different persistence diagrams that must be documented.

Stability testing protocols validate features across scales through systematic perturbation analysis. The practical protocol involves generating n perturbed datasets by adding Gaussian noise (starting with σ = 0.05× elevation range, increasing to 0.1, 0.2, 0.35), computing persistence diagrams for each, calculating mean bottleneck distances, and identifying stable features appearing in ≥95% of perturbed samples. The multiresolution testing framework constructs a stability matrix with rows representing different resolutions, columns representing different preprocessing parameters, and entries containing bottleneck distances between diagrams—stable features form consistent clusters in this matrix. Studies should test at minimum five different resolutions covering from the finest available to 4-5× the coarsest expected feature size, using resolution ratios of 1.5-2.0 between successive levels.

## Appalachian physiographic provinces provide an ideal testbed with established ground truth

The five major Appalachian physiographic provinces—Coastal Plain, Piedmont, Blue Ridge, Valley and Ridge, and Appalachian Plateau—represent a hierarchy of terrain complexity ideal for validating TDA methods because they have authoritative boundaries, distinct quantitative signatures, and span the full range from simple to complex topography. The USGS Fenneman physiographic provinces dataset (Fenneman and Johnson 1946) provides the authoritative ground truth at 1:7,000,000 scale, available through USGS ScienceBase as vector polygons with attributes including DIVISION, PROVINCE, SECTION, FENCODE, and PROVCODE. While this coarse scale limits local analysis, it establishes the reference standard that any automated classification method must match or exceed.

Quantitative methods for distinguishing these provinces have evolved from traditional discriminant analysis to sophisticated machine learning approaches. Random Forest consistently achieves the highest accuracy (**92-99.3% overall accuracy**, Kappa 0.83-0.99) across multiple terrain classification studies, with Support Vector Machine following closely at 84-99.7% and K-Nearest Neighbor at 90-99.3%. For climate-driven geomorphic provinces in analogous terrain, these methods achieved exceptional performance: RF at 99.27% overall accuracy (Kappa 0.99) and SVM at 99.70%, though Appalachian-specific studies show more modest but still strong performance around 86-96%. The critical insight for TDA validation is that **any novel method should target ≥90% overall accuracy with Kappa ≥0.85** to be competitive with established approaches, with the recognition that transition zones and complex provinces like Valley and Ridge may achieve only 85-90% due to inherent boundary uncertainty.

The discriminant geomorphometric parameters reveal what topological analysis must capture to succeed. Feature importance analysis consistently ranks **elevation as the single most important discriminator**, followed by valley depth, slope, drainage density, and local relief. The provinces exhibit distinct quantitative signatures: Coastal Plain shows low elevation (near sea level), minimal slope, very low drainage density (\u003c2 km/km²), and negligible relief; Piedmont displays moderate elevation (300-1000 feet), rolling topography, and moderate drainage density (1-3 km/km²); Blue Ridge presents high elevation (\u003e1000-6600 feet), steep slopes (mean 17°), high relief (867m range documented in North Carolina), and strong topographic position contrasts; Valley and Ridge exhibits the most distinctive pattern with alternating linear ridges and valleys trending NE-SW, ~1000 feet typical relief, high drainage density in valleys, and strong aspect patterns; Appalachian Plateau shows a dissected plateau surface with moderate to high elevation (500-4862 feet), relatively flat-lying strata, variable drainage density, and distinct escarpments like the Allegheny Front.

Sample size recommendations from machine learning literature establish concrete targets for TDA validation studies. The ASPRS standard requires minimum 20 checkpoints per major landform category, but optimal performance requires substantially more: **800-1200 training samples per province** (4,000-6,000 total for five provinces) and **400-600 validation samples per province** (2,000-3,000 total) using stratified random sampling across province extent and terrain variability. Studies demonstrate that accuracy plateaus after ~800-1000 samples per class, while below 300 samples per class causes significant performance degradation (5-10% accuracy loss). Remarkably, Bhuyan et al. (2024) showed that TDA methods achieved \u003e70% accuracy with only 100 samples per class in data-rich Italy and \u003e75% accuracy with \u003c20 samples in data-scarce Wenchuan basins, suggesting topological features may require fewer training samples than conventional methods—a hypothesis requiring systematic testing.

The validation datasets available provide multiple quality tiers for rigorous evaluation. Primary elevation data comes from the USGS 3D Elevation Program (3DEP) offering 1-meter LiDAR-derived DEMs with **10cm vertical accuracy (RMSEz) in non-vegetated areas** for areas with QL2 LiDAR coverage, 10-meter DEMs from 1/3 arc-second data nationally, and 30-meter DEMs from SRTM with complete coverage. State geological surveys provide refined province boundaries at scales more appropriate for detailed analysis: West Virginia Geological and Economic Survey integrates LiDAR with geology, Virginia Department of Energy provides mineral resources mapping with physiographic context, and North Carolina's floodplain mapping program offers 5-meter LiDAR DEMs for the Blue Ridge Province. The global MERIT DEM (90m resolution) and derived Geomorpho90m dataset (26 standardized terrain variables) enable continental-scale comparisons and provide baseline terrain attributes for method benchmarking.

## Baseline geomorphometry establishes the performance bar TDA must exceed

The current state-of-the-art for landscape classification centers on three complementary approaches that any TDA method must match or surpass. Geomorphons (Jasiewicz and Stepinski 2013) use pattern recognition based on local ternary patterns rather than differential geometry, identifying **498 unique geomorphon patterns** reduced from 6,561 by removing rotations and reflections, classifying terrain into 10 standard landform classes (flat, peak, ridge, shoulder, spur, slope, hollow, footslope, valley, pit). The method is self-adapting to identify optimal spatial scale at each location and achieves unprecedented computational efficiency for giga-cell DEMs. Implementation in GRASS GIS (r.geomorphon), ArcGIS Pro 3.1+, and SAGA GIS requires setting search distance (50-100× cell size for large features, 5-15× for small features), skip distance (0-10 cells to reduce noise), and flat threshold (1-2 degrees). Comparison between MERIT-DEM and 3DEP-1 geomorphons shows 70-86% agreement depending on landform type, with highest agreement for flat terrain (86%) and moderate agreement for ridges and summits (~40-60%), demonstrating that even established methods struggle with complex transitional landforms.

The Geomorpho90m framework (Amatulli et al. 2020) standardizes 26 terrain variables that represent current best practice: 11 first derivatives including slope, aspect, eastness, northness, convergence index, Compound Topographic Index, Stream Power Index, and directional derivatives; 5 second derivatives capturing profile curvature, tangential curvature, and second directional derivatives; 9 ruggedness measures including elevation standard deviation, Terrain Ruggedness Index, roughness, Vector Ruggedness Measure, Topographic Position Index, and multiscale deviation/roughness metrics; plus the geomorphon classification. This comprehensive suite based on MERIT-DEM at 90m global resolution provides the baseline against which TDA features must be compared—studies should minimally include the Tier 1 essential parameters (slope, CTI, profile curvature, geomorphons/TPI) and ideally the full 10-20 parameter standard set to ensure fair comparison.

Machine learning classification accuracies with these baseline methods establish concrete performance targets. Random Forest on Mars terrain achieved **94.66% overall accuracy** (highest among KNN, SVM, RF), while applications to Earth terrain consistently achieve 85-95% for well-designed studies. Support Vector Machine typically performs 2-5% below Random Forest at 84-97%, and deep learning methods using Convolutional Neural Networks achieve **84-95% overall accuracy** with FCN-ResNet architecture for landform semantic segmentation, or 87-95% when combining DEM with imagery. Performance varies substantially by terrain class: flat terrain achieves 86-95% accuracy, compacted soil 93-95%, rocky terrain 87-89%, valleys and gullies 85-90%, but complex transitional landforms drop to 70-85%. This establishes that **\u003e90% accuracy represents excellent performance, 85-90% is good for complex terrain, and \u003c70% is unacceptable** unless justified by extreme terrain complexity or data limitations.

Directional and anisotropic terrain analysis methods provide capabilities that isotropic TDA approaches may miss. The Median Absolute Differences (MAD) operator (Trevisani and Rocca 2015) provides robust estimation of directional surface roughness that is less sensitive to outliers than traditional variograms and captures multiscale and anisotropic character when applied to residual DTMs. Fourier transform methods identify characteristic spatial frequencies through 2D discrete FFT power spectrum analysis, achieving **82% accuracy for landslide morphology detection** in Puget Sound by identifying hummocky topography, scarps, and displaced blocks through spectral density functions describing magnitude of periodicities, spacing, overall roughness, and contrast between large and small feature slopes. Hyperscale topographic anisotropy using the DEV metric with integral image filtering samples nearly continuous ranges of scales with scale-independent time complexity, self-selecting optimal window sizes per grid cell and measuring directional inequality in the elevation field—capabilities that TDA must match through appropriate vectorization of directional topological features.

Fair comparison protocols require meticulous attention to validation design. The confusion matrix analysis provides the foundation: overall accuracy (total correct / total samples), producer's accuracy for omission errors, user's accuracy for commission errors, and Kappa coefficient measuring agreement beyond chance (though Foody 2020 argues Kappa should be abandoned due to prevalence sensitivity and interpretability issues, recommending Matthews Correlation Coefficient instead). Stratified random sampling remains the gold standard, with minimum 30-50 points per class but 250-300 total points typical for large areas, ensuring representativeness across class areas and terrain variability. The critical requirement for TDA comparison is using **identical input DEMs, identical preprocessing, identical validation points, and identical cross-validation folds** across all methods being compared, with reporting of all parameters, software versions, and algorithmic choices to enable reproducibility.

## Statistical rigor for spatial data fundamentally changes validation requirements

Spatial autocorrelation in terrain data creates the single most serious threat to validation validity that most TDA studies completely ignore. The landmark study by Ploton et al. (2020) in Nature Communications demonstrated the magnitude of this problem using central African forest data: random 10-fold cross-validation yielded R²=0.53, appearing to show strong predictive power, but spatial cross-validation revealed R²=0.14 (near-null predictive ability). The models predicted based on spatial proximity rather than environmental relationships, a pattern invisible to standard diagnostic methods because random cross-validation completely absorbed the spatial autocorrelation into the residuals. Roberts et al. (2017) established that ecological and geospatial data violate independent and identically distributed (IID) assumptions, causing **28-40% inflation of performance estimates**, false confidence in model transferability, and failure to detect overfitting.

Spatial block cross-validation represents the mandatory minimum validation approach for any terrain classification study. The method divides data into spatially contiguous blocks using hierarchical clustering or grid-based partitioning, with **block size determined by the autocorrelation range** obtained through variogram analysis or Moran's I calculation at multiple distance lags. The critical rule established by Valavi et al. (2019) and confirmed across multiple studies is that **block size must exceed 1.5-2× the spatial autocorrelation range**—smaller blocks fail to eliminate spatial leakage between training and test sets. Implementation through the blockCV R package supports random, systematic ("snake"), and checkerboard block assignment, handling presence-absence and presence-background data appropriately while ensuring balanced class representation within blocks. Alternative approaches include buffered leave-one-out cross-validation that excludes training data within radius r around each test observation (revealing how predictive power varies with spatial distance through testing multiple buffer radii from 0 to 150+ km), and the novel Spatial+ cross-validation (2023, ISPRS Journal) that considers both geographic AND feature space through two-stage clustering to ensure test sets represent the full environmental gradient.

The methodological failure documented across the literature demands immediate correction. Stock (2025) in Frontiers in Remote Sensing demonstrated that even with optimal spatial blocks, model selection bias may not be completely eliminated—the best strategy mirrors actual data structure such as leaving out whole watersheds or physiographic regions. Meyer et al. (2019) revealed severe overfitting risks when geographic coordinates are included as features: highly autocorrelated latitude and longitude enable data reproduction rather than spatial prediction, creating clear visual artifacts in prediction maps unrelated to true processes. The solution requires spatial variable selection to automatically detect and remove overfitting variables. Liu et al. (2022) showed that spatial lag features (weighted averages of neighboring observations) and eigenvector spatial filtering can reduce errors by up to 33% and decrease residual Moran's I by 95%, but must be applied thoughtfully as they fundamentally change what the model represents.

Multiple testing corrections become essential when comparing TDA against multiple baseline methods. Family-Wise Error Rate control through Bonferroni correction (α_adjusted = α/m for m tests) or the less conservative Holm-Bonferroni step-down procedure controls the probability of any false positive and should be used when false positives are costly, such as in operational mapping decisions. False Discovery Rate control through the Benjamini-Hochberg procedure controls the expected proportion of false discoveries among rejections and proves more powerful for exploratory research with many comparisons (\u003e20 tests). For the typical case of comparing TDA against 3-5 baseline methods (Random Forest, SVM, geomorphons, CNN), the Holm-Bonferroni procedure provides the appropriate balance between Type I error control and statistical power.

Effect size reporting and accuracy assessment standards have shifted dramatically in the last five years. Stehman and Foody (2023) critique that remote sensing deep learning papers have largely abandoned traditional accuracy assessment, rarely reporting complete confusion matrices or population-based estimates. The mandatory reporting elements now include: complete confusion/error matrices (not just summary statistics), per-class accuracy metrics (producer's accuracy/recall, user's accuracy/precision, F1-score per class), overall metrics with both macro-averaged and micro-averaged calculations, detailed sampling design description (how validation points were selected, sample size justification, spatial distribution), uncertainty quantification (confidence intervals, standard errors), and spatial validation results showing performance by geographic region and distance-dependent performance. The controversial recommendation from Foody (2020) to abandon Cohen's Kappa entirely due to its sensitivity to prevalence, class imbalance, and difficult interpretation (same accuracy can yield κ ranging from -0.026 to 0.900) has gained substantial traction, with Matthews Correlation Coefficient proposed as the more stable alternative.

Power analysis protocols for spatial classification studies must account for both sample size and spatial configuration. The effective sample size formula n_eff = n / (1 + (n-1)ρ), where ρ represents average correlation between observations, demonstrates how autocorrelation reduces statistical power—a study with 1000 samples but ρ=0.5 has effective sample size of only 334. Simulation-based power estimation remains the most reliable approach: generate synthetic landscapes with known properties matching expected spatial structure, apply the proposed sampling design, run classification with spatial cross-validation, repeat 1000+ iterations, and calculate power as the proportion yielding significant results. The sparrpowR R package (Buller et al. 2021) provides spatial power analysis for cluster detection through 10,000+ iterations with user-defined parameters, while the in silico tissue generation framework from Nature Methods 2023 optimizes number of samples, fields of view, and their sizes. Studies should target **80% power to detect the minimum meaningful effect size** while accounting for spatial autocorrelation, and report sensitivity analyses showing how power varies with autocorrelation assumptions and spatial sampling configurations.

## Practical implementation requires synthesis across validation domains

Recent literature on TDA applications to terrain reveals both remarkable successes and critical limitations that inform validation strategy. The Bhuyan et al. (2024) study represents the current gold standard, demonstrating that six optimal topological features—average lifetime of holes (ALH, 22% feature importance), average lifetime of connected components (ALC, 18%), Betti curve-based features for components and holes (BCC 15%, BCH), Wasserstein amplitude of holes (WAH, 12%), and bottleneck amplitude of holes (BAH, 8%)—achieved 94% micro F1-score for landslide movement classification in Italy. The geographic transferability testing across four continents with different failure mechanisms, geologies, and climates achieved 80-94% accuracy, while temporal transferability on Wenchuan multi-temporal inventories (2008-2018) exceeded 90% accuracy. Critically, they used 10-fold cross-validation repeated 1000 times with standard deviation \u003c0.2%, demonstrating exceptional robustness.

The vectorization methods employed determine whether topological features can integrate with machine learning pipelines. Persistence landscapes (Bubenik 2015) provide functional summaries in Hilbert space L²(N×R) that obey the strong law of large numbers and central limit theorem, enabling standard statistical tests with well-developed theory for computing means, variances, and confidence bands. Implementation through PersLandscapeExact stores critical points exactly, while PersLandscapeApprox samples onto grids for computational efficiency. Persistence images (Adams et al. 2017) create finite-dimensional vectors by discretizing persistence diagrams onto grids (typically 20×20 to 150×150 resolution) with weighting functions, providing stability under perturbations and seamless integration with CNNs and SVMs. Bhuyan et al. used the Giotto-TDA library computing persistence landscapes and derived statistics, selecting 6 optimal features from 30 candidates through correlation analysis and feature importance ranking—topological features consistently outperformed geometric descriptors, with even the least important topological feature (BCC at 15%) comparable to geometric features that collectively achieved only ~65% accuracy.

The documented failure modes establish clear boundaries on TDA applicability. Syzdykbayev et al. (2024) demonstrated that applying only topological information produces notable false positives requiring integration with geometrical filters (area, perimeter, convexity) and contextual filters (slope, vegetation, soil moisture). Performance variations by landslide characteristics included sensitivity to surface roughness (smooth vs. rough terrain), landslide age (older vegetated features harder to detect), size variations (very small or large features challenging), and reactivation history (multiple events complicate signatures). Computational complexity remains limiting: computing persistent homology is O(exp(N)) for full datasets, requiring subsampling to O(n·exp(m)) where m \u003c\u003c N. Parameter sensitivity demands careful tuning of filtration methods, resolution/grid size, and smoothing parameters with no universal settings across applications. The fundamental limitation that 3D topology doesn't directly connect to failure processes—relying solely on shape rather than underlying physics—means TDA cannot distinguish anthropogenic from natural features with similar morphology or infer causative processes from topology alone.

Software implementations have matured substantially with comprehensive tools now available. Giotto-TDA provides the most complete Python implementation with scikit-learn API compatibility; Ripser offers fast C++/Python persistence computation; GUDHI supplies comprehensive C++/Python general-purpose TDA functionality; the TDA R package emphasizes statistical methods with landscapes and kernel distances; TDAvec (R/Python) provides efficient vectorization **\u003e200× faster** than the base TDA package; persim (Python) specializes in persistence images and landscapes; and domain-specific tools like JavaPlex for education, Dionysus, Perseus, PHAT, and DIPHA for specialized applications. The blockCV, sperrorest, mlr3spatiotempcv, and CAST R packages provide spatial cross-validation implementations, while sparrpowR enables spatial power analysis. This mature ecosystem eliminates software barriers—implementation failures now reflect methodological rather than technical limitations.

The integrated validation workflow synthesizes requirements across domains into a practical protocol. The pre-study phase conducts spatial structure assessment by computing variograms for response and predictor variables, calculating Moran's I at multiple scales, determining autocorrelation range, and conducting power analysis for the proposed sampling design. The model development phase implements spatial feature engineering (considering spatial lag or eigenvector spatial filtering), screens for overfitting variables especially coordinates, and uses nested cross-validation with spatial blocks for hyperparameter tuning (inner loop) and performance assessment (outer loop). The model comparison phase tests multiple methods with identical spatial CV folds enabling paired tests, applies Holm-Bonferroni correction for \u003c10 comparisons or Benjamini-Hochberg for \u003e10 comparisons, reports both raw and adjusted p-values, and assesses performance through complete confusion matrices, per-class metrics, spatial error distribution maps, and distance-dependent performance via buffered LOO CV. The reporting phase documents spatial CV approach and block size rationale, autocorrelation assessment results, multiple testing correction methods, full confusion matrices with confidence intervals, comparison with random CV to demonstrate bias, error distribution maps, spatial transferability limitations, geographic regions where models fail, and implications for map uncertainty.

## Key principles for defensible TDA applications in geosciences

Successful validation of TDA for terrain analysis requires abandoning the fiction that topological methods work in isolation and embracing integration with conventional terrain attributes. The evidence is unambiguous: topological information alone produces unacceptable false positive rates (Syzdykbayev et al. 2024), but when integrated with geometrical and contextual features, achieves state-of-the-art performance (94% accuracy, Bhuyan et al. 2024) that transfers geographically and temporally. The validation framework must include multiple independent verification streams: ground truth comparison against authoritative inventories, multi-sample consistency testing where features must appear across subsamples, geographic transferability testing across regions with different terrain characteristics, temporal transferability for multi-temporal datasets, and bootstrap methods for confidence band construction. Preprocessing sensitivity analysis is not optional—it must systematically test resolution sensitivity across minimum five scales with logarithmic spacing, noise robustness through Distance-to-Measure approaches, filtration parameter sensitivity varying neighborhood radii and bandwidth values, and alternative complex constructions (Čech vs. Vietoris-Rips vs. alpha) with documented stability across parameter changes.

Spatial cross-validation represents the methodological line in the sand that separates defensible from indefensible validation. The dramatic example from Ploton et al. (2020) showing apparent R²=0.53 with random CV collapsing to true R²=0.14 with spatial CV, combined with documented 28-50% performance inflation across multiple studies, establishes that random CV for spatial data is scientific malpractice. Block size must exceed 1.5-2× the spatial autocorrelation range determined through variogram analysis, implemented through blockCV or equivalent with comparison to random CV explicitly showing the bias. Multiple testing corrections apply when comparing methods: Holm-Bonferroni for moderate comparisons (\u003c10), Benjamini-Hochberg for many comparisons (\u003e20), with both raw and adjusted p-values reported. Complete confusion matrices with per-class metrics are mandatory—overall accuracy alone masks failures, and Cohen's Kappa should likely be abandoned in favor of Matthews Correlation Coefficient due to prevalence sensitivity. Effect sizes must include confidence intervals, spatial error distribution maps, and distance-dependent performance assessment.

For Appalachian physiographic province classification specifically, the concrete requirements are: high-resolution DEMs (1-10m LiDAR-derived preferred, minimum 30m acceptable), validation against USGS Fenneman provinces as authoritative ground truth supplemented by state geological survey boundaries, 800-1200 training samples per province (4,000-6,000 total) with 400-600 validation samples per province (2,000-3,000 total) using stratified random sampling, core terrain parameters (elevation, valley depth, slope, drainage density, TPI) as baseline comparison, target performance \u003e90% overall accuracy with Kappa \u003e0.85 to be competitive with Random Forest baselines achieving 92-99%, recognition that transition zones and complex provinces will achieve 85-90% with boundary uncertainty acknowledged, and spatial blocks designed to respect province boundaries or major sub-basins.

The comparison against baseline geomorphometry methods must meet rigorous standards: use identical input DEMs with identical preprocessing for all methods, employ the same spatial CV folds across all methods enabling paired statistical tests, include minimally the Tier 1 essential baseline (slope, CTI, profile curvature, geomorphons/TPI) and preferably standard 10-parameter suite from Geomorpho90m, test Random Forest as primary baseline (consistently achieving 92-95% for well-designed terrain studies), include geomorphons for pattern-based comparison and deep learning CNN methods if computational resources permit, report processing time and computational complexity alongside accuracy for efficiency comparison, document all parameters and provide code for reproducibility, and demonstrate what TDA captures that conventional methods miss through explicit feature importance analysis and case studies of successfully vs. unsuccessfully classified regions.

Sample size requirements and power analysis must account for spatial structure. While conventional wisdom suggests 800-1000 samples per class, Bhuyan et al.'s demonstration of \u003e70% accuracy with only 100 samples per class and \u003e75% with \u003c20 samples in data-scarce regions suggests topological features may require fewer training samples—a hypothesis requiring systematic testing but offering hope for data-limited applications. Power analysis must use simulation-based approaches generating synthetic landscapes with known properties, applying proposed sampling designs, running classification with spatial CV, repeating 1000+ iterations, and calculating required sample size for 80% power while varying autocorrelation assumptions. The effective sample size formula n_eff = n / (1 + (n-1)ρ) provides estimates, but simulation-based approaches prove more reliable for complex spatial designs.

Critical success factors synthesized across all domains establish the minimum viable validation: never use topology alone but always integrate with geometrical and contextual features, always use spatial cross-validation with blocks \u003e1.5× autocorrelation range, always report complete confusion matrices with per-class metrics and spatial error distributions, always apply multiple testing corrections when comparing methods, always test geographic transferability across multiple regions before claiming generalizability, always conduct preprocessing sensitivity analysis documenting stability across resolution and parameter changes, always compare against state-of-the-art baselines (Random Forest, geomorphons) using identical validation protocols, and always provide complete parameter documentation and reproducible code. Studies failing these criteria should not pass peer review regardless of claimed accuracy.

The path forward requires community consensus on standardized protocols. The field needs benchmark datasets with labeled landforms across diverse terrain types, standardized test areas with multiple DEM qualities and expert classifications enabling method comparison, agreed-upon reporting standards for spatial classification studies that journals enforce consistently, and software tools integrating spatial CV into standard workflows making proper validation the default rather than exceptional. The cost of ignoring spatial structure and accepting validation based on random CV is too high—producing maps with illusory accuracy that guide misguided land management decisions, emergency response planning, and scientific conclusions. The validation framework synthesized here provides the foundation for rigorous, defensible TDA applications that can advance geomorphometry while maintaining the statistical and methodological integrity the field demands.