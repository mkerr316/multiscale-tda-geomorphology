{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "15fc8f9fd55ab2b7",
      "metadata": {
        "id": "15fc8f9fd55ab2b7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import scipy\n",
        "from scipy.spatial import distance_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import random\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e6f7a14",
      "metadata": {},
      "outputs": [],
      "source": [
        "#%pip install gudhi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be63e666ae445ef3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be63e666ae445ef3",
        "outputId": "5e54a23b-ff60-4bd2-be98-072c6883684b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gudhi in /opt/conda/envs/app/lib/python3.11/site-packages (3.11.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/envs/app/lib/python3.11/site-packages (from gudhi) (2.3.4)\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import gudhi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "def kwargValidation(cls: str, func : str, **kwargs):\n",
        "        with open('/kwargsVerification.JSON', 'r') as file:\n",
        "            verification = json.load(file)\n",
        "            funcVer = verification[cls][func]\n",
        "            for key in funcVer[\"Variables\"]:\n",
        "                if key in kwargs.keys():\n",
        "                    if str(type(kwargs[key])) != funcVer[\"Types\"][key]:\n",
        "                        raise Exception(\"The given %s variable is not of the correct type\" %key)\n",
        "                    else:\n",
        "                        valList = [eval(condition) for condition in funcVer[\"Validation\"][key]]\n",
        "                        valList = [condition(kwargs[key]) for condition in valList]\n",
        "                        if not all(valList):\n",
        "                            raise Exception(\"The given %s variable does not satisfy the condition for this variable\" %key)\n",
        "                else:\n",
        "                    try:\n",
        "                        kwargs[key] = eval(funcVer[\"Default\"][key])(kwargs)\n",
        "                    except Exception as e:\n",
        "                        print(e)\n",
        "                        kwargs[key] = funcVer[\"Default\"][key]\n",
        "\n",
        "\n",
        "        return kwargs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91a19042815286fb",
      "metadata": {
        "id": "91a19042815286fb"
      },
      "source": [
        "# The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "71519203fb88a579",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71519203fb88a579",
        "outputId": "aa86cdc2-e76f-40c2-cc6e-35378b0de6a6"
      },
      "outputs": [],
      "source": [
        "#%pip install persim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f485094",
      "metadata": {},
      "outputs": [],
      "source": [
        "from persim import PersistenceImager as pimg\n",
        "class TopologicalFeatures_Detection:\n",
        "    def __init__(self, **kwargs):\n",
        "        kwargs = kwargValidation(self.__class__.__name__,\"init\", **kwargs)\n",
        "        for key, value in kwargs.items():\n",
        "            setattr(self, key, value)\n",
        "        return\n",
        "\n",
        "    def __heightMatrix(self, angle: float, size: int) -> np.ndarray:\n",
        "        heightMatrix = np.zeros((size, size), np.float32)\n",
        "        step = self.scale/size\n",
        "        for i in range(size):\n",
        "            for j in range(size):\n",
        "                heightMatrix[i, j] = float(np.cos(angle)*((size-i)*step+ -1) + np.sin(angle)*((j)*step -1))\n",
        "        return heightMatrix\n",
        "\n",
        "    def detect(self, image: np.ndarray, size: int, alpha: float = 0.5) -> tuple:\n",
        "        pers_imager = pimg(pixel_size=.1,birth_range=(-np.sqrt(2)-.01,np.sqrt(2)+.01))\n",
        "        H0_Block = []\n",
        "        H1_Block = []\n",
        "        for direction in self.directions:\n",
        "            heightMatrix = self.__heightMatrix(direction, size)\n",
        "            Image = np.round((image > alpha)).reshape((size,size))\n",
        "            heightImgMatrix = Image * heightMatrix\n",
        "            heightImgMatrix[heightImgMatrix == 0] = float('inf')\n",
        "            Complex = gudhi.CubicalComplex(top_dimensional_cells=heightImgMatrix)\n",
        "            Complex.persistence()\n",
        "            D0 = Complex.persistence_intervals_in_dimension(0)\n",
        "            D1 = Complex.persistence_intervals_in_dimension(1)\n",
        "            D0[D0 == float('inf')] = self.scale*np.sqrt(2)+.01\n",
        "            D1[D1 == float('inf')] = self.scale*np.sqrt(2)+.01\n",
        "            pers_img_0 = pers_imager.transform(D0)\n",
        "            pers_img_1 = pers_imager.transform(D1)\n",
        "            H0_Block.append(pers_img_0.reshape(pers_img_0.shape[0]*pers_img_0.shape[1],))\n",
        "            H1_Block.append(pers_img_1.reshape(pers_img_0.shape[0]*pers_img_0.shape[1],))\n",
        "        return (H0_Block, H1_Block)\n",
        "\n",
        "    def matching(self, SensedtopInformation, RefTopInformation) -> list:\n",
        "        cycles = [[(i+j) % self.numOfDirections for i in range(self.numOfDirections)] for j in range(self.numOfDirections)]\n",
        "        probs = []\n",
        "        for ref in RefTopInformation:\n",
        "            D0 = distance_matrix(ref[0],SensedtopInformation[0])\n",
        "            D1 = distance_matrix(ref[1],SensedtopInformation[1])\n",
        "            BestCycle = None\n",
        "            currentMin = float('inf')\n",
        "            for cycle in cycles:\n",
        "                score = sum(D0[[i for i in range(self.numOfDirections)],cycle]*D0[[i for i in range(self.numOfDirections)],cycle])+sum(D1[[i for i in range(self.numOfDirections)],cycle]*D1[[i for i in range(self.numOfDirections)],cycle])/(2*self.numOfDirections)\n",
        "                if score < currentMin:\n",
        "                    BestCycle = cycle\n",
        "                    currentMin = score\n",
        "            probs.append([currentMin,BestCycle])\n",
        "        return probs\n",
        "\n",
        "    def train(self, training_dataset, training_label, RefTopInformation, size) -> None:\n",
        "        TopInfo = list(map(lambda x: self.detect(x,size),training_dataset))\n",
        "        matchingProbs = list(map(lambda x: self.matching(x,RefTopInformation), TopInfo))\n",
        "        model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "        model.fit(matchingProbs, training_label)\n",
        "        self.model = model\n",
        "        return\n",
        "\n",
        "    def evaluate(self, test_dataset, test_label, RefTopInformation, size) -> None:\n",
        "        TopInfo = list(map(lambda x: self.detect(x,size),test_dataset))\n",
        "        matchingProbs = list(map(lambda x: self.matching(x,RefTopInformation), TopInfo))\n",
        "        print(self.model.score(matchingProbs, test_label))\n",
        "        return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5c9e8716b5ecc16",
      "metadata": {
        "id": "f5c9e8716b5ecc16"
      },
      "source": [
        "# Step One: Feature Detection\n",
        "In this step we want to detect topological feature by \"scanning\" the image in different direction. We do this by considering a height function h<sub>v<sub>i</sub></sub>(x) = v<sub>i</sub> $\\cdot$ x for i = 1,...,N. Each i will produce a persistence diagram and hence a persistence image (i.e a vector) summerizing the topological features in that direction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc19f3c80767c9a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc19f3c80767c9a7",
        "outputId": "637f2639-b9af-4875-b713-0aad33d9d50e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.78MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 155kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.45MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.77MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download and transform the MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    # Convert images to PyTorch tensors which also scales data from [0,255] to [0,1]\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Download training and test datasets\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "568ee786d40f6511",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "568ee786d40f6511",
        "outputId": "65da9c2a-3eaf-4273-b842-dc0c52118a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting opencv-python\n",
            "  Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
            "Collecting numpy<2.3.0,>=2 (from opencv-python)\n",
            "  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, opencv-python\n",
            "\u001b[2K  Attempting uninstall: numpy\n",
            "\u001b[2K    Found existing installation: numpy 2.3.4\n",
            "\u001b[2K    Uninstalling numpy-2.3.4:\n",
            "\u001b[2K      Successfully uninstalled numpy-2.3.4\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [opencv-python]0m [opencv-python]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ripser 0.6.12 requires Cython, which is not installed.\n",
            "numpy-typing-compat 20250818.2.3 requires numpy<2.4,>=2.3, but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.6 opencv-python-4.12.0.88\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install opencv-python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ac120e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import scipy.ndimage as ndi\n",
        "T = TopologicalFeatures_Detection(numOfDirections = 8)\n",
        "Image, Label = train_dataset[0]\n",
        "Image = cv2.resize(src=np.array(Image).reshape(28,28),dsize=(50, 50),interpolation=cv2.INTER_CUBIC)\n",
        "cy, cx = ndi.center_of_mass(Image)\n",
        "t = np.float32([[1, 0, 25-cx], [0, 1, 25-cy]])\n",
        "\n",
        "Image = cv2.warpAffine(Image, t, (50, 50))\n",
        "cy, cx = ndi.center_of_mass(Image)\n",
        "\n",
        "print(cy, cx)\n",
        "H0_demo, H1_demo = T.detect(Image,50)\n",
        "plt.imshow((Image).reshape(50,50))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6ee8143d1c4233c",
      "metadata": {
        "id": "c6ee8143d1c4233c"
      },
      "source": [
        "# Step Two: Feature Matching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a846ee59a3c18bde",
      "metadata": {
        "id": "a846ee59a3c18bde"
      },
      "outputs": [],
      "source": [
        "train_dataset_images = []\n",
        "train_dataset_label = []\n",
        "\n",
        "for image, label in train_dataset:\n",
        "    train_dataset_images.append(np.array(image).reshape(28,28))\n",
        "    train_dataset_label.append(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7edde5450fa4e50",
      "metadata": {
        "id": "c7edde5450fa4e50"
      },
      "outputs": [],
      "source": [
        "RefPics = []\n",
        "\n",
        "new_train_dataset = []\n",
        "new_train_dataset_label = []\n",
        "\n",
        "for i in range(9):\n",
        "    ind = np.where(np.array(train_dataset_label) == i)[0]\n",
        "    j = random.choice(list(ind))\n",
        "    image = train_dataset_images[j]\n",
        "    H0, H1 = T.detect(image,28)\n",
        "    RefPics.append([H0,H1])\n",
        "\n",
        "    #rotIndex = []\n",
        "    #for j in range(20):\n",
        "        #angle = random.uniform(0,360)\n",
        "        #while angle in tempIndex:\n",
        "            #angle = random.uniform(0,360)\n",
        "        #rotIndex.append(refIndex)\n",
        "        #new_train_dataset.append(scipy.ndimage.rotate(image, angle, reshape=False))\n",
        "        #new_train_dataset_label.append(train_dataset_label[ind])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c906c266170dda1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "9c906c266170dda1",
        "outputId": "de2506a4-ed36-4f91-bff0-c322580baf7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25.29343865064919 23.753213825262907\n",
            "[[np.float64(0.0005548993058222125), [0, 1, 2, 3, 4, 5, 6, 7]]]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import cv2\n",
        "rotation_matrix = cv2.getRotationMatrix2D((25,25), 135, 1)\n",
        "Image2 , Label2= train_dataset[1]\n",
        "Image2 = cv2.resize(src=np.array(Image2).reshape(28,28),dsize=(50, 50),interpolation=cv2.INTER_CUBIC)\n",
        "cy, cx = ndi.center_of_mass(Image2)\n",
        "t = np.float32([[1, 0, 25-cx], [0, 1, 25-cy]])\n",
        "rotated_image = cv2.warpAffine(src=np.array(Image2).reshape(50,50), M=rotation_matrix, dsize=(50, 50))\n",
        "\n",
        "rotated_image = cv2.warpAffine(rotated_image, t, (50, 50))\n",
        "cy, cx = ndi.center_of_mass(rotated_image)\n",
        "print(cy, cx)\n",
        "plt.imshow(rotated_image)\n",
        "\n",
        "H0_demo_rot, H1_demo_rot = T.detect(rotated_image,50)\n",
        "\n",
        "Probs = T.matching([H0_demo, H1_demo], [[H0_demo_rot, H1_demo_rot]])\n",
        "print(Probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34883c18e166574e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "34883c18e166574e",
        "outputId": "11974908-29d1-4be5-cb96-39069e87d5ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Model...\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "index 4 is out of bounds for axis 0 with size 4",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-218-dd069a3adbd6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataset_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mRefPics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating Model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataset_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mRefPics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-212-80a321f8a084>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_dataset, training_label, RefTopInformation, size)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRefTopInformation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mTopInfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mmatchingProbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mRefTopInformation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTopInfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'multinomial'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatchingProbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-212-80a321f8a084>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRefTopInformation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mTopInfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mmatchingProbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mRefTopInformation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTopInfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'multinomial'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatchingProbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-212-80a321f8a084>\u001b[0m in \u001b[0;36mmatching\u001b[0;34m(self, SensedtopInformation, RefTopInformation)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mcurrentMin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcycle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcycles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumOfDirections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcycle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mD0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumOfDirections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcycle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumOfDirections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcycle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mD1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumOfDirections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcycle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumOfDirections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mcurrentMin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                     \u001b[0mBestCycle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for axis 0 with size 4"
          ]
        }
      ],
      "source": [
        "print(\"Training Model...\")\n",
        "T.train(train_dataset_images[0:500],train_dataset_label[0:500],RefPics,28)\n",
        "print(\"Evaluating Model...\")\n",
        "T.evaluate(train_dataset_images[0:100],train_dataset_label[0:100],RefPics,28)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
